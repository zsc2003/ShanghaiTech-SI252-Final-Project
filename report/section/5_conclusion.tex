\section{Conclusion} \label{sec:5_conclusion}

\paragraph{Stochastic Bandit}

This study presents a stochastic multi-armed bandit algorithm based on policy gradients, augmented with a discrete diffusion model to generate offline trajectory data and thereby enrich a limited dataset. We conduct a comprehensive simulation by varying multiple factors—source of offline data, reward distributions per arm (Bernoulli vs. non-Bernoulli), trajectory-generation strategies, and the number of trajectories. Results show that, regardless of whether the rewards follow a Bernoulli or a non-Bernoulli distribution, the policy-gradient approach delivers strong performance in stochastic bandit environments. Moreover, when the offline dataset—initially collected with UCB or Thompson Sampling—is enhanced by diffusion-generated trajectories, the final average regret is substantially reduced.

Our experiments further demonstrate that incorporating temporal structure, particularly via a Transformer-based discrete diffusion model, more effectively captures sequential and time-series patterns in the data, producing trajectories that closely match those generated by the true policy. As the number of diffusion-generated trajectories increases, the benefits of pre-training become even more pronounced, revealing excellent scalability and generalization ability.

In summary, the proposed Transformer-based discrete diffusion trajectory generation method yields significant pre-training advantages in stochastic multi-armed bandit problems. It not only mitigates sample scarcity during policy learning but also markedly lowers average regret, offering a practical and efficient solution for policy optimization in offline reinforcement learning settings.


\paragraph{Contextual Bandit}

We presented a diffusion-based approach to contextual bandits that models the full conditional reward distribution instead of relying solely on mean estimation. While our method performs competitively and outperforms several UCB-based baselines, it underperforms compared to simpler mean-based strategies such as NeuralTS and $\varepsilon$-Greedy in binary reward settings. This highlights a trade-off: the expressive power of distributional modeling may be underutilized when the reward structure is simple.
Looking forward, we plan to design contextual bandit environments with richer reward distributions, e.g., bimodal or heavy-tailed, where capturing higher-order uncertainty is essential. Such settings will better showcase the potential of diffusion-based methods in modeling complex decision uncertainty beyond mean reward estimates.
