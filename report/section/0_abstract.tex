\begin{abstract}
Offline multi-armed bandit methods are frequently constrained by sparse or distributionally incomplete historical logs. This work introduces discrete diffusion models as principled generators that synthesize additional, distribution-consistent samples to avoid lacking data. We then used the discrete diffusion model in two canonical settings. In the stochastic bandit, a diffusion model fitted to offline trajectories expands the dataset without further interaction; on a newly defined class of non-Bernoulli reward functions where Thompson Sampling is inconsistent, diffusion-augmented logs gathered under UCB still deliver competitive policies. In the contextual bandit, a conditional diffusion model produces arm-specific reward distributions conditioned on context, enabling sampling-based decisions that capture uncertainty beyond the mean. Across both tasks, the discrete diffusion model achieves significant results.
\end{abstract}
