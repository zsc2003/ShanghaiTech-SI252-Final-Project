\section{Related Work} \label{sec:2_related_work}

\paragraph{Stochastic Bandit}
The stochastic multi-armed bandit seeks an optimal explorationâ€“exploitation balance over a finite set of arms. Classical solutions include the optimistic Upper Confidence Bound (UCB)~\cite{UCB} family and the Bayesian method Thompson Sampling~\cite{TS}, which couples exploration and exploitation through posterior sampling. In contrast to these confidence-based or posterior-based randomisation schemes, the gradient bandit~\cite{sutton1999policy} directly performs ascent on a parameterised stochastic policy, continuously adapting the arm-selection probabilities.

As large-scale applications raise demands for generalisation and sample efficiency, attention has shifted to offline bandits. When the available logs are small, however, reliance on estimators alone can be unreliable, inspiring the use of generative models to enlarge data coverage: variational autoencoders~\cite{VAE} and GANs~\cite{GAN}, and recently most famous diffusion models~\cite{ddpm} have already been applied to synthesise supplementary samples for offline reinforcement learning. Discrete diffusion models, with their stable training and high sample fidelity, now offer a promising alternative for data augmentation in offline bandits. Following this line, the present study trains a discrete diffusion model on limited interaction logs in the stochastic bandit setting, generates pseudo-trajectories consistent with the empirical distribution, and employs the augmented dataset to warm-start both UCB, Thompson Sampling, and gradient-based algorithms. Experiments confirm that these diffusion-generated samples markedly reduce initial regret and accelerate convergence.



\paragraph{Contextual Bandit}
Contextual bandit algorithms address sequential decision-making problems where, at each round, an agent observes a context, selects an action, and receives a stochastic reward. The aim is to maximize cumulative reward by learning the reward-generating relationship between context-action pairs.  The methods for modeling this relationship have progressively advanced from simple linear formulations to more expressive nonparametric and deep learning-based approaches. In the linear setting, Li et al.\cite{li_contextual-bandit_2010} proposed LinUCB and Agrawal and Goyal \cite{agrawal_thompson_2014} developed LinTS, both of which assume a linear reward model over context-action features and construct confidence bounds around parameter estimates. More general nonlinear bandits without making strong modeling assumptions have also be considered. Srinivas et al.\cite{srinivas_gaussian_nodate} introduced GP-UCB, and Valko et al. \cite{valko_finite-time_2013} proposed KernelUCB, extending the hypothesis class to reproducing kernel Hilbert spaces (RKHS), enabling more expressive reward representations. More recently, neural network-based methods such as NeuralUCB \cite{zhou_neural_2020} and NeuralTS \cite{zhang_neural_2021} have employed deep learning architectures to approximate complex reward functions, incorporating uncertainty through gradient-based proxies or auxiliary variance estimation. Despite these advances, the primary objective of these methods remains unchanged: to estimate the conditional expectation of the reward given context and action, without explicitly modeling the structure of the full reward distribution.