# ShanghaiTech-SI252-Final-Project
ShanghaiTech SI252 Reinforcement Learning final project, Spring 2025.

# Weekly Report

## week 1 (2025/5/18)
- Contextual bandit: We've finished the basic code for fitting the reward distribution using discrete diffusion. The initial results look okay, but online training hasnâ€™t shown much improvement yet, so we still need to tweak the training strategy.

- Stochastic bandit: We've finished the basic code for generating the trajectory: $\psi=((a_1,r_1),\dots,(a_T,r_T))$. And the corresponding code with different discrete diffusion methods to enlarge the offline dataset. Current method for generating the trajectory is using the Bernoulli reward, and Thomson sampling. And the generation methods are suing generating by pair, generating by sequence, and generating by sequence with transformer encoder. More generation methods and non-Bernoulli reward are under discovering.


## week 2 (2025/5/25)

- Contextual bandit: We've finished experimenting with various replay buffer strategies this week, aiming to emphasize the impact of negative samples on the correction of diffusion parameters, ultimately intensifying the effect of online updates. We also included comparisons with other methods such as KernelUCB, NeuralUCB, and NeuralEpsilon. However, due to the more generalized nature of diffusion, its adjustment speed is slower than that of NeuralEpsilon. Next week, we plan to improve the internal sampling method of the diffusion model.

- Stochastic bandit: We have finished the code for the trajectory generation with different methods: simply duplicate the trajectory, generate by pair, generate by sequence, generate by sequence with transformer encoder. The non-Bernoulli reward is also tested, in this case, the origin offline dataset's trajectory is generated by UCB's online interaction. In this case, Thompson sampling algorithm is no longer suitable due to lack of conjugate prior. The reward function is set to be $X=\begin{cases}
0 & \text{ w.p. } \theta_1 \\
0.5 & \text{ w.p. } \theta_2 \\
1 & \text{ w.p. } 1-\theta_1-\theta_2 \\
\end{cases}$ \
More comparison and analysis are under discovering.


## week 3 (2025/6/1)
- Contextual bandit: We replaced the internal mechanism of the discrete diffusion version of the RL project with a more precise alternative to mask version, but the results did not improve.

- Stochastic bandit: We did further explorations on the stochastic bandit problem. With different settings: different number of arms, different hyperparameters, different types of reward distributions. \
More research work such as combining with restless bandit, converting dream policy's idea into a stochastic bandit problem, backing up the codes are the TODOs.
