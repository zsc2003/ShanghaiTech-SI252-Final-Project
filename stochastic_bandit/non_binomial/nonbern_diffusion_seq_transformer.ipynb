{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 trajectories of length 50\n",
      "Epoch 1/400  loss=4.1533\n",
      "Epoch 2/400  loss=3.9911\n",
      "Epoch 3/400  loss=3.9071\n",
      "Epoch 4/400  loss=3.7577\n",
      "Epoch 5/400  loss=3.6779\n",
      "Epoch 6/400  loss=3.5161\n",
      "Epoch 7/400  loss=3.2334\n",
      "Epoch 8/400  loss=3.0790\n",
      "Epoch 9/400  loss=3.0732\n",
      "Epoch 10/400  loss=2.9027\n",
      "Epoch 11/400  loss=2.6193\n",
      "Epoch 12/400  loss=2.5785\n",
      "Epoch 13/400  loss=2.3282\n",
      "Epoch 14/400  loss=2.2457\n",
      "Epoch 15/400  loss=2.1216\n",
      "Epoch 16/400  loss=2.0677\n",
      "Epoch 17/400  loss=1.8163\n",
      "Epoch 18/400  loss=1.8947\n",
      "Epoch 19/400  loss=1.8056\n",
      "Epoch 20/400  loss=1.6391\n",
      "Epoch 21/400  loss=1.4570\n",
      "Epoch 22/400  loss=1.3717\n",
      "Epoch 23/400  loss=1.3218\n",
      "Epoch 24/400  loss=1.3595\n",
      "Epoch 25/400  loss=1.2665\n",
      "Epoch 26/400  loss=1.2089\n",
      "Epoch 27/400  loss=1.2819\n",
      "Epoch 28/400  loss=1.1702\n",
      "Epoch 29/400  loss=1.1845\n",
      "Epoch 30/400  loss=1.1843\n",
      "Epoch 31/400  loss=0.9685\n",
      "Epoch 32/400  loss=1.0723\n",
      "Epoch 33/400  loss=1.0634\n",
      "Epoch 34/400  loss=1.0401\n",
      "Epoch 35/400  loss=0.8875\n",
      "Epoch 36/400  loss=0.9092\n",
      "Epoch 37/400  loss=0.7903\n",
      "Epoch 38/400  loss=0.9098\n",
      "Epoch 39/400  loss=0.9194\n",
      "Epoch 40/400  loss=0.9182\n",
      "Epoch 41/400  loss=0.8201\n",
      "Epoch 42/400  loss=0.8596\n",
      "Epoch 43/400  loss=0.8477\n",
      "Epoch 44/400  loss=0.7312\n",
      "Epoch 45/400  loss=0.8519\n",
      "Epoch 46/400  loss=0.8953\n",
      "Epoch 47/400  loss=0.8834\n",
      "Epoch 48/400  loss=0.7599\n",
      "Epoch 49/400  loss=0.7403\n",
      "Epoch 50/400  loss=0.7072\n",
      "Epoch 51/400  loss=0.7073\n",
      "Epoch 52/400  loss=0.8649\n",
      "Epoch 53/400  loss=0.7556\n",
      "Epoch 54/400  loss=0.8232\n",
      "Epoch 55/400  loss=0.8563\n",
      "Epoch 56/400  loss=0.7806\n",
      "Epoch 57/400  loss=0.7068\n",
      "Epoch 58/400  loss=0.7445\n",
      "Epoch 59/400  loss=0.7919\n",
      "Epoch 60/400  loss=0.7786\n",
      "Epoch 61/400  loss=0.7078\n",
      "Epoch 62/400  loss=0.8216\n",
      "Epoch 63/400  loss=0.7462\n",
      "Epoch 64/400  loss=0.8337\n",
      "Epoch 65/400  loss=0.6737\n",
      "Epoch 66/400  loss=0.8611\n",
      "Epoch 67/400  loss=0.6890\n",
      "Epoch 68/400  loss=0.7928\n",
      "Epoch 69/400  loss=0.8203\n",
      "Epoch 70/400  loss=0.7608\n",
      "Epoch 71/400  loss=0.6655\n",
      "Epoch 72/400  loss=0.8151\n",
      "Epoch 73/400  loss=0.7767\n",
      "Epoch 74/400  loss=0.8240\n",
      "Epoch 75/400  loss=0.7559\n",
      "Epoch 76/400  loss=0.7085\n",
      "Epoch 77/400  loss=0.7126\n",
      "Epoch 78/400  loss=0.7784\n",
      "Epoch 79/400  loss=0.6976\n",
      "Epoch 80/400  loss=0.7044\n",
      "Epoch 81/400  loss=0.7716\n",
      "Epoch 82/400  loss=0.7017\n",
      "Epoch 83/400  loss=0.7165\n",
      "Epoch 84/400  loss=0.6394\n",
      "Epoch 85/400  loss=0.6586\n",
      "Epoch 86/400  loss=0.8194\n",
      "Epoch 87/400  loss=0.7047\n",
      "Epoch 88/400  loss=0.7319\n",
      "Epoch 89/400  loss=0.7237\n",
      "Epoch 90/400  loss=0.6578\n",
      "Epoch 91/400  loss=0.8530\n",
      "Epoch 92/400  loss=0.5653\n",
      "Epoch 93/400  loss=0.8004\n",
      "Epoch 94/400  loss=0.8640\n",
      "Epoch 95/400  loss=0.7347\n",
      "Epoch 96/400  loss=0.7984\n",
      "Epoch 97/400  loss=0.8086\n",
      "Epoch 98/400  loss=0.9137\n",
      "Epoch 99/400  loss=0.7026\n",
      "Epoch 100/400  loss=0.6549\n",
      "Epoch 101/400  loss=0.8096\n",
      "Epoch 102/400  loss=0.5282\n",
      "Epoch 103/400  loss=0.7367\n",
      "Epoch 104/400  loss=0.8610\n",
      "Epoch 105/400  loss=0.5794\n",
      "Epoch 106/400  loss=0.9037\n",
      "Epoch 107/400  loss=0.6679\n",
      "Epoch 108/400  loss=0.6080\n",
      "Epoch 109/400  loss=0.8888\n",
      "Epoch 110/400  loss=0.5870\n",
      "Epoch 111/400  loss=0.6493\n",
      "Epoch 112/400  loss=0.7743\n",
      "Epoch 113/400  loss=0.6149\n",
      "Epoch 114/400  loss=0.6145\n",
      "Epoch 115/400  loss=0.7783\n",
      "Epoch 116/400  loss=0.7043\n",
      "Epoch 117/400  loss=0.6160\n",
      "Epoch 118/400  loss=0.7475\n",
      "Epoch 119/400  loss=0.6832\n",
      "Epoch 120/400  loss=0.8305\n",
      "Epoch 121/400  loss=0.6959\n",
      "Epoch 122/400  loss=0.6997\n",
      "Epoch 123/400  loss=0.7400\n",
      "Epoch 124/400  loss=0.7262\n",
      "Epoch 125/400  loss=0.7989\n",
      "Epoch 126/400  loss=0.9975\n",
      "Epoch 127/400  loss=0.6759\n",
      "Epoch 128/400  loss=0.6460\n",
      "Epoch 129/400  loss=0.7156\n",
      "Epoch 130/400  loss=0.9214\n",
      "Epoch 131/400  loss=0.6763\n",
      "Epoch 132/400  loss=0.6792\n",
      "Epoch 133/400  loss=0.8266\n",
      "Epoch 134/400  loss=0.8221\n",
      "Epoch 135/400  loss=0.8585\n",
      "Epoch 136/400  loss=0.6984\n",
      "Epoch 137/400  loss=0.8510\n",
      "Epoch 138/400  loss=0.7589\n",
      "Epoch 139/400  loss=0.6098\n",
      "Epoch 140/400  loss=0.6473\n",
      "Epoch 141/400  loss=0.8040\n",
      "Epoch 142/400  loss=0.8895\n",
      "Epoch 143/400  loss=0.7443\n",
      "Epoch 144/400  loss=0.7341\n",
      "Epoch 145/400  loss=0.8491\n",
      "Epoch 146/400  loss=0.5912\n",
      "Epoch 147/400  loss=0.9290\n",
      "Epoch 148/400  loss=0.8585\n",
      "Epoch 149/400  loss=0.8140\n",
      "Epoch 150/400  loss=0.7173\n",
      "Epoch 151/400  loss=0.6825\n",
      "Epoch 152/400  loss=0.6169\n",
      "Epoch 153/400  loss=0.7105\n",
      "Epoch 154/400  loss=0.7369\n",
      "Epoch 155/400  loss=0.8112\n",
      "Epoch 156/400  loss=0.5703\n",
      "Epoch 157/400  loss=0.7342\n",
      "Epoch 158/400  loss=0.7789\n",
      "Epoch 159/400  loss=0.6955\n",
      "Epoch 160/400  loss=0.8354\n",
      "Epoch 161/400  loss=0.7613\n",
      "Epoch 162/400  loss=0.8388\n",
      "Epoch 163/400  loss=0.5812\n",
      "Epoch 164/400  loss=0.7602\n",
      "Epoch 165/400  loss=0.8227\n",
      "Epoch 166/400  loss=0.6614\n",
      "Epoch 167/400  loss=0.8269\n",
      "Epoch 168/400  loss=0.7383\n",
      "Epoch 169/400  loss=0.6914\n",
      "Epoch 170/400  loss=0.9082\n",
      "Epoch 171/400  loss=0.7791\n",
      "Epoch 172/400  loss=0.7616\n",
      "Epoch 173/400  loss=0.9180\n",
      "Epoch 174/400  loss=0.7676\n",
      "Epoch 175/400  loss=0.8276\n",
      "Epoch 176/400  loss=0.6976\n",
      "Epoch 177/400  loss=0.7272\n",
      "Epoch 178/400  loss=0.8148\n",
      "Epoch 179/400  loss=0.6497\n",
      "Epoch 180/400  loss=0.6312\n",
      "Epoch 181/400  loss=0.8083\n",
      "Epoch 182/400  loss=0.7158\n",
      "Epoch 183/400  loss=0.8551\n",
      "Epoch 184/400  loss=0.7764\n",
      "Epoch 185/400  loss=0.8373\n",
      "Epoch 186/400  loss=0.6783\n",
      "Epoch 187/400  loss=0.8775\n",
      "Epoch 188/400  loss=0.6695\n",
      "Epoch 189/400  loss=0.9097\n",
      "Epoch 190/400  loss=0.7436\n",
      "Epoch 191/400  loss=0.6367\n",
      "Epoch 192/400  loss=0.6838\n",
      "Epoch 193/400  loss=0.9282\n",
      "Epoch 194/400  loss=0.7208\n",
      "Epoch 195/400  loss=0.7094\n",
      "Epoch 196/400  loss=0.6424\n",
      "Epoch 197/400  loss=0.6929\n",
      "Epoch 198/400  loss=0.7097\n",
      "Epoch 199/400  loss=0.9294\n",
      "Epoch 200/400  loss=0.7547\n",
      "Epoch 201/400  loss=0.7085\n",
      "Epoch 202/400  loss=0.7845\n",
      "Epoch 203/400  loss=0.9368\n",
      "Epoch 204/400  loss=0.6674\n",
      "Epoch 205/400  loss=0.7445\n",
      "Epoch 206/400  loss=0.7624\n",
      "Epoch 207/400  loss=0.8192\n",
      "Epoch 208/400  loss=0.7130\n",
      "Epoch 209/400  loss=0.8483\n",
      "Epoch 210/400  loss=0.8120\n",
      "Epoch 211/400  loss=0.9612\n",
      "Epoch 212/400  loss=0.6406\n",
      "Epoch 213/400  loss=0.6921\n",
      "Epoch 214/400  loss=0.7599\n",
      "Epoch 215/400  loss=0.9023\n",
      "Epoch 216/400  loss=0.5263\n",
      "Epoch 217/400  loss=0.8040\n",
      "Epoch 218/400  loss=0.7005\n",
      "Epoch 219/400  loss=0.7895\n",
      "Epoch 220/400  loss=0.8237\n",
      "Epoch 221/400  loss=0.8217\n",
      "Epoch 222/400  loss=0.8121\n",
      "Epoch 223/400  loss=0.8752\n",
      "Epoch 224/400  loss=0.8458\n",
      "Epoch 225/400  loss=0.8045\n",
      "Epoch 226/400  loss=0.6956\n",
      "Epoch 227/400  loss=0.6843\n",
      "Epoch 228/400  loss=0.7242\n",
      "Epoch 229/400  loss=0.7601\n",
      "Epoch 230/400  loss=0.8275\n",
      "Epoch 231/400  loss=0.8834\n",
      "Epoch 232/400  loss=0.7023\n",
      "Epoch 233/400  loss=0.6894\n",
      "Epoch 234/400  loss=0.7706\n",
      "Epoch 235/400  loss=0.7381\n",
      "Epoch 236/400  loss=0.6993\n",
      "Epoch 237/400  loss=0.6687\n",
      "Epoch 238/400  loss=0.6664\n",
      "Epoch 239/400  loss=0.8982\n",
      "Epoch 240/400  loss=0.6593\n",
      "Epoch 241/400  loss=0.6657\n",
      "Epoch 242/400  loss=0.6828\n",
      "Epoch 243/400  loss=0.7233\n",
      "Epoch 244/400  loss=0.7578\n",
      "Epoch 245/400  loss=0.7871\n",
      "Epoch 246/400  loss=0.8282\n",
      "Epoch 247/400  loss=0.8985\n",
      "Epoch 248/400  loss=0.8615\n",
      "Epoch 249/400  loss=0.6739\n",
      "Epoch 250/400  loss=0.8899\n",
      "Epoch 251/400  loss=0.6661\n",
      "Epoch 252/400  loss=0.7451\n",
      "Epoch 253/400  loss=0.6251\n",
      "Epoch 254/400  loss=0.6290\n",
      "Epoch 255/400  loss=0.7735\n",
      "Epoch 256/400  loss=0.8524\n",
      "Epoch 257/400  loss=0.6750\n",
      "Epoch 258/400  loss=0.6427\n",
      "Epoch 259/400  loss=0.8205\n",
      "Epoch 260/400  loss=0.7431\n",
      "Epoch 261/400  loss=0.8220\n",
      "Epoch 262/400  loss=0.7596\n",
      "Epoch 263/400  loss=0.8530\n",
      "Epoch 264/400  loss=0.6908\n",
      "Epoch 265/400  loss=0.5840\n",
      "Epoch 266/400  loss=0.7400\n",
      "Epoch 267/400  loss=0.9371\n",
      "Epoch 268/400  loss=0.7457\n",
      "Epoch 269/400  loss=0.6560\n",
      "Epoch 270/400  loss=0.6276\n",
      "Epoch 271/400  loss=0.7191\n",
      "Epoch 272/400  loss=1.0177\n",
      "Epoch 273/400  loss=1.0029\n",
      "Epoch 274/400  loss=0.6345\n",
      "Epoch 275/400  loss=0.7425\n",
      "Epoch 276/400  loss=0.6753\n",
      "Epoch 277/400  loss=0.5851\n",
      "Epoch 278/400  loss=0.6130\n",
      "Epoch 279/400  loss=0.8140\n",
      "Epoch 280/400  loss=0.8286\n",
      "Epoch 281/400  loss=0.7318\n",
      "Epoch 282/400  loss=0.8371\n",
      "Epoch 283/400  loss=0.7154\n",
      "Epoch 284/400  loss=0.5962\n",
      "Epoch 285/400  loss=0.7403\n",
      "Epoch 286/400  loss=0.7346\n",
      "Epoch 287/400  loss=0.6855\n",
      "Epoch 288/400  loss=0.7696\n",
      "Epoch 289/400  loss=0.5493\n",
      "Epoch 290/400  loss=0.7831\n",
      "Epoch 291/400  loss=0.8145\n",
      "Epoch 292/400  loss=0.7051\n",
      "Epoch 293/400  loss=0.7751\n",
      "Epoch 294/400  loss=0.7654\n",
      "Epoch 295/400  loss=0.7383\n",
      "Epoch 296/400  loss=0.7854\n",
      "Epoch 297/400  loss=0.7080\n",
      "Epoch 298/400  loss=0.7636\n",
      "Epoch 299/400  loss=0.6566\n",
      "Epoch 300/400  loss=0.6590\n",
      "Epoch 301/400  loss=0.7231\n",
      "Epoch 302/400  loss=0.7425\n",
      "Epoch 303/400  loss=0.8006\n",
      "Epoch 304/400  loss=0.7344\n",
      "Epoch 305/400  loss=0.7405\n",
      "Epoch 306/400  loss=0.8167\n",
      "Epoch 307/400  loss=0.6678\n",
      "Epoch 308/400  loss=0.6897\n",
      "Epoch 309/400  loss=0.5464\n",
      "Epoch 310/400  loss=0.7425\n",
      "Epoch 311/400  loss=0.7112\n",
      "Epoch 312/400  loss=0.8016\n",
      "Epoch 313/400  loss=0.7885\n",
      "Epoch 314/400  loss=0.8024\n",
      "Epoch 315/400  loss=0.8648\n",
      "Epoch 316/400  loss=0.7794\n",
      "Epoch 317/400  loss=0.7252\n",
      "Epoch 318/400  loss=0.7665\n",
      "Epoch 319/400  loss=0.8473\n",
      "Epoch 320/400  loss=0.7545\n",
      "Epoch 321/400  loss=0.6305\n",
      "Epoch 322/400  loss=0.6345\n",
      "Epoch 323/400  loss=0.7570\n",
      "Epoch 324/400  loss=0.6642\n",
      "Epoch 325/400  loss=0.6696\n",
      "Epoch 326/400  loss=0.8760\n",
      "Epoch 327/400  loss=0.6233\n",
      "Epoch 328/400  loss=0.6035\n",
      "Epoch 329/400  loss=0.6414\n",
      "Epoch 330/400  loss=0.8043\n",
      "Epoch 331/400  loss=0.7469\n",
      "Epoch 332/400  loss=0.8410\n",
      "Epoch 333/400  loss=0.8575\n",
      "Epoch 334/400  loss=0.6298\n",
      "Epoch 335/400  loss=0.8835\n",
      "Epoch 336/400  loss=0.7808\n",
      "Epoch 337/400  loss=0.8160\n",
      "Epoch 338/400  loss=0.6189\n",
      "Epoch 339/400  loss=0.7078\n",
      "Epoch 340/400  loss=0.7305\n",
      "Epoch 341/400  loss=0.7240\n",
      "Epoch 342/400  loss=0.6628\n",
      "Epoch 343/400  loss=0.6254\n",
      "Epoch 344/400  loss=0.6838\n",
      "Epoch 345/400  loss=0.7457\n",
      "Epoch 346/400  loss=0.6354\n",
      "Epoch 347/400  loss=0.7775\n",
      "Epoch 348/400  loss=0.7523\n",
      "Epoch 349/400  loss=0.7633\n",
      "Epoch 350/400  loss=0.8104\n",
      "Epoch 351/400  loss=0.7559\n",
      "Epoch 352/400  loss=0.7871\n",
      "Epoch 353/400  loss=0.7354\n",
      "Epoch 354/400  loss=0.8448\n",
      "Epoch 355/400  loss=0.7786\n",
      "Epoch 356/400  loss=0.6789\n",
      "Epoch 357/400  loss=0.8084\n",
      "Epoch 358/400  loss=0.6640\n",
      "Epoch 359/400  loss=0.7594\n",
      "Epoch 360/400  loss=0.7091\n",
      "Epoch 361/400  loss=0.6357\n",
      "Epoch 362/400  loss=0.7627\n",
      "Epoch 363/400  loss=0.6839\n",
      "Epoch 364/400  loss=0.7712\n",
      "Epoch 365/400  loss=0.7786\n",
      "Epoch 366/400  loss=0.7821\n",
      "Epoch 367/400  loss=0.6836\n",
      "Epoch 368/400  loss=0.7918\n",
      "Epoch 369/400  loss=0.7622\n",
      "Epoch 370/400  loss=0.6821\n",
      "Epoch 371/400  loss=0.6378\n",
      "Epoch 372/400  loss=0.6200\n",
      "Epoch 373/400  loss=0.8218\n",
      "Epoch 374/400  loss=0.6491\n",
      "Epoch 375/400  loss=0.7412\n",
      "Epoch 376/400  loss=0.7400\n",
      "Epoch 377/400  loss=0.6909\n",
      "Epoch 378/400  loss=0.6362\n",
      "Epoch 379/400  loss=0.9442\n",
      "Epoch 380/400  loss=0.9351\n",
      "Epoch 381/400  loss=0.7599\n",
      "Epoch 382/400  loss=0.7640\n",
      "Epoch 383/400  loss=0.8020\n",
      "Epoch 384/400  loss=0.6464\n",
      "Epoch 385/400  loss=0.7390\n",
      "Epoch 386/400  loss=0.7319\n",
      "Epoch 387/400  loss=0.6096\n",
      "Epoch 388/400  loss=0.7154\n",
      "Epoch 389/400  loss=0.7669\n",
      "Epoch 390/400  loss=0.8364\n",
      "Epoch 391/400  loss=0.7023\n",
      "Epoch 392/400  loss=0.5997\n",
      "Epoch 393/400  loss=0.7312\n",
      "Epoch 394/400  loss=0.9002\n",
      "Epoch 395/400  loss=0.7096\n",
      "Epoch 396/400  loss=0.8100\n",
      "Epoch 397/400  loss=0.6720\n",
      "Epoch 398/400  loss=0.7815\n",
      "Epoch 399/400  loss=0.7764\n",
      "Epoch 400/400  loss=0.8314\n",
      "Saved generated trajectories: (600, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Imports & Config\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import torch, math, random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "K = 20\n",
    "NUM_SAMPLES = 500\n",
    "input_path   = f'traj_{K}_nonbern.npy'\n",
    "output_path  = f'traj_{K}_nonbern_diff.npy'\n",
    "\n",
    "\n",
    "BATCH_SIZE   = 10\n",
    "EPOCHS       = 400\n",
    "T_STEPS      = 50          # diffusion steps\n",
    "LR           = 2e-4\n",
    "EMBED_DIM    = 64\n",
    "N_LAYERS     = 4\n",
    "N_HEADS      = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_R   = 3\n",
    "NUM_TOK = K * NUM_R        # 每个 time-step 的组合 token 种数\n",
    "# =========================================================\n",
    "# 2. Data : (N, L, 2) → (N, L) integer tokens\n",
    "# =========================================================\n",
    "raw = np.load(input_path)               # (N, L, 2)\n",
    "SEQ_LEN = raw.shape[1]\n",
    "print('Loaded', raw.shape[0], 'trajectories of length', SEQ_LEN)\n",
    "a_arr = raw[:, :, 0].astype(np.int64)\n",
    "\n",
    "# r_arr: 0 -> 0, 0.5 -> 1, 1 -> 2\n",
    "r_arr = (raw[:, :, 1] * 2).astype(np.int64)\n",
    "\n",
    "tok_arr = (a_arr * NUM_R + r_arr).astype(np.int64)   # (N, L)\n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tok = torch.from_numpy(tokens).long()\n",
    "    def __len__(self):\n",
    "        return self.tok.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tok[idx]          # (L,)\n",
    "dataloader = DataLoader(TrajDataset(tok_arr), batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=True)\n",
    "# =========================================================\n",
    "# 3. Discrete forward diffusion helper (token → token)\n",
    "# =========================================================\n",
    "def forward_diffusion(x0, betas, num_classes):\n",
    "    \"\"\"x0 : (B, L) LongTensor → list len T+1, each (B, L)\"\"\"\n",
    "    traj = [x0]\n",
    "    x_prev = x0\n",
    "    for beta in betas:\n",
    "        mask  = (torch.rand_like(x_prev.float()) < beta)\n",
    "        noise = torch.randint(0, num_classes, x_prev.shape, device=x_prev.device)\n",
    "        x_next = torch.where(mask, noise, x_prev)\n",
    "        traj.append(x_next)\n",
    "        x_prev = x_next\n",
    "    return traj\n",
    "# =========================================================\n",
    "# 4. Model : sequence‐aware discrete diffusion network\n",
    "# =========================================================\n",
    "class SeqDiscreteDiffusion(nn.Module):\n",
    "    def __init__(self, num_tok=NUM_TOK, seq_len=SEQ_LEN, embed_dim=EMBED_DIM, n_layers=N_LAYERS, n_heads=N_HEADS):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_tok, embed_dim)\n",
    "        self.pos_emb   = nn.Embedding(seq_len, embed_dim)\n",
    "        self.time_emb  = nn.Embedding(1000, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim*4, activation='relu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(embed_dim, num_tok)\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t : (B, L)  LongTensor\n",
    "        t   : (B,)    LongTensor  (same t for the whole seq)\n",
    "        \"\"\"\n",
    "        B, L = x_t.shape\n",
    "        tok  = self.token_emb(x_t)                         # (B,L,E)\n",
    "        pos  = self.pos_emb(torch.arange(L, device=x_t.device))  # (L,E)\n",
    "        pos  = pos.unsqueeze(0).expand(B, -1, -1)          # (B,L,E)\n",
    "        time = self.time_emb(t).unsqueeze(1).expand(-1, L, -1)   # (B,L,E)\n",
    "        h = tok + pos + time                               # (B,L,E)\n",
    "        h = self.transformer(h)                            # (B,L,E)\n",
    "        logits = self.head(h)                              # (B,L,NUM_TOK)\n",
    "        return logits\n",
    "# =========================================================\n",
    "# 5. Training loop\n",
    "# =========================================================\n",
    "betas = [0.1] * T_STEPS\n",
    "model = SeqDiscreteDiffusion().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for x0 in dataloader:          # x0 : (B,L)\n",
    "        x0 = x0.to(device)\n",
    "        B, L = x0.shape\n",
    "        # forward diffusion once per batch (global t for the whole seq)\n",
    "        traj  = forward_diffusion(x0, betas, NUM_TOK)      # list len T+1\n",
    "        t_bar = torch.randint(1, T_STEPS + 1, (B,), device=device)  # (B,)\n",
    "        x_t   = torch.stack([traj[t][i]   for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        x_prev= torch.stack([traj[t-1][i] for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        logits = model(x_t, t_bar)                         # (B,L,NUM_TOK)\n",
    "        loss   = ce(logits.reshape(-1, NUM_TOK), x_prev.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}  loss={loss.item():.4f}')\n",
    "# =========================================================\n",
    "# 6. Sampling (reverse diffusion)\n",
    "# =========================================================\n",
    "def sample(model, n_samples=NUM_SAMPLES):\n",
    "    model.eval()\n",
    "    B = n_samples\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randint(0, NUM_TOK, (B, SEQ_LEN), device=device)\n",
    "        for t in reversed(range(1, T_STEPS + 1)):\n",
    "            t_vec = torch.full((B,), t, device=device)\n",
    "            logits = model(x_t, t_vec)                     # (B,L,NUM_TOK)\n",
    "            probs  = torch.softmax(logits, dim=-1)\n",
    "            x_t    = torch.multinomial(probs.view(-1, NUM_TOK), 1).squeeze(-1).view(B, SEQ_LEN)      # (B,L)\n",
    "        return x_t.cpu()                                   # (B,L)\n",
    "gen_tok = sample(model, NUM_SAMPLES)                       # (N,L)\n",
    "# 解码回 (a,r) 二元组\n",
    "gen_a = (gen_tok // NUM_R).numpy()\n",
    "gen_r = (gen_tok %  NUM_R).numpy().astype(np.float64) / 2\n",
    "gen_traj = np.stack([gen_a, gen_r], axis=-1)               # (N,L,2)\n",
    "# concat the training data `raw`` and the generated `data gen_traj`\n",
    "trajs = np.concatenate([raw, gen_traj], axis=0)          # (N+200, L, 2)\n",
    "np.save(output_path, trajs)\n",
    "print('Saved generated trajectories:', trajs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
