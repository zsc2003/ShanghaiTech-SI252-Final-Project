{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 trajectories of length 50\n",
      "Epoch 1/400  loss=3.6158\n",
      "Epoch 2/400  loss=3.4958\n",
      "Epoch 3/400  loss=3.4029\n",
      "Epoch 4/400  loss=3.2742\n",
      "Epoch 5/400  loss=3.1443\n",
      "Epoch 6/400  loss=3.0134\n",
      "Epoch 7/400  loss=2.8561\n",
      "Epoch 8/400  loss=2.7214\n",
      "Epoch 9/400  loss=2.4771\n",
      "Epoch 10/400  loss=2.4155\n",
      "Epoch 11/400  loss=2.2263\n",
      "Epoch 12/400  loss=2.1071\n",
      "Epoch 13/400  loss=1.8736\n",
      "Epoch 14/400  loss=1.7328\n",
      "Epoch 15/400  loss=1.5178\n",
      "Epoch 16/400  loss=1.3790\n",
      "Epoch 17/400  loss=1.3065\n",
      "Epoch 18/400  loss=1.2609\n",
      "Epoch 19/400  loss=1.1407\n",
      "Epoch 20/400  loss=1.0336\n",
      "Epoch 21/400  loss=1.1026\n",
      "Epoch 22/400  loss=1.0092\n",
      "Epoch 23/400  loss=0.9455\n",
      "Epoch 24/400  loss=0.9566\n",
      "Epoch 25/400  loss=0.8417\n",
      "Epoch 26/400  loss=0.8242\n",
      "Epoch 27/400  loss=0.9314\n",
      "Epoch 28/400  loss=0.7196\n",
      "Epoch 29/400  loss=0.8482\n",
      "Epoch 30/400  loss=0.9097\n",
      "Epoch 31/400  loss=0.8121\n",
      "Epoch 32/400  loss=0.9606\n",
      "Epoch 33/400  loss=0.9939\n",
      "Epoch 34/400  loss=0.6629\n",
      "Epoch 35/400  loss=0.8916\n",
      "Epoch 36/400  loss=0.9096\n",
      "Epoch 37/400  loss=0.7225\n",
      "Epoch 38/400  loss=0.9316\n",
      "Epoch 39/400  loss=0.7947\n",
      "Epoch 40/400  loss=0.9702\n",
      "Epoch 41/400  loss=0.8558\n",
      "Epoch 42/400  loss=0.7419\n",
      "Epoch 43/400  loss=0.7691\n",
      "Epoch 44/400  loss=0.8449\n",
      "Epoch 45/400  loss=0.8214\n",
      "Epoch 46/400  loss=0.7911\n",
      "Epoch 47/400  loss=0.6774\n",
      "Epoch 48/400  loss=0.7081\n",
      "Epoch 49/400  loss=0.8998\n",
      "Epoch 50/400  loss=0.7116\n",
      "Epoch 51/400  loss=0.7817\n",
      "Epoch 52/400  loss=0.8244\n",
      "Epoch 53/400  loss=0.8504\n",
      "Epoch 54/400  loss=0.7161\n",
      "Epoch 55/400  loss=0.7579\n",
      "Epoch 56/400  loss=0.7697\n",
      "Epoch 57/400  loss=0.7574\n",
      "Epoch 58/400  loss=0.8518\n",
      "Epoch 59/400  loss=0.5842\n",
      "Epoch 60/400  loss=0.9147\n",
      "Epoch 61/400  loss=0.7625\n",
      "Epoch 62/400  loss=0.7568\n",
      "Epoch 63/400  loss=0.7476\n",
      "Epoch 64/400  loss=0.7693\n",
      "Epoch 65/400  loss=0.6463\n",
      "Epoch 66/400  loss=0.7594\n",
      "Epoch 67/400  loss=0.7646\n",
      "Epoch 68/400  loss=0.8421\n",
      "Epoch 69/400  loss=0.8351\n",
      "Epoch 70/400  loss=0.7244\n",
      "Epoch 71/400  loss=0.7289\n",
      "Epoch 72/400  loss=0.7916\n",
      "Epoch 73/400  loss=0.6983\n",
      "Epoch 74/400  loss=0.9129\n",
      "Epoch 75/400  loss=0.7387\n",
      "Epoch 76/400  loss=0.8376\n",
      "Epoch 77/400  loss=0.6840\n",
      "Epoch 78/400  loss=0.8406\n",
      "Epoch 79/400  loss=0.7896\n",
      "Epoch 80/400  loss=0.7442\n",
      "Epoch 81/400  loss=0.7356\n",
      "Epoch 82/400  loss=0.6583\n",
      "Epoch 83/400  loss=0.8044\n",
      "Epoch 84/400  loss=0.8110\n",
      "Epoch 85/400  loss=0.8175\n",
      "Epoch 86/400  loss=0.6411\n",
      "Epoch 87/400  loss=0.7548\n",
      "Epoch 88/400  loss=0.7054\n",
      "Epoch 89/400  loss=0.7270\n",
      "Epoch 90/400  loss=0.8735\n",
      "Epoch 91/400  loss=0.8432\n",
      "Epoch 92/400  loss=0.8465\n",
      "Epoch 93/400  loss=0.8201\n",
      "Epoch 94/400  loss=0.6889\n",
      "Epoch 95/400  loss=0.8066\n",
      "Epoch 96/400  loss=0.8272\n",
      "Epoch 97/400  loss=0.7446\n",
      "Epoch 98/400  loss=0.7258\n",
      "Epoch 99/400  loss=0.7725\n",
      "Epoch 100/400  loss=0.7338\n",
      "Epoch 101/400  loss=0.7121\n",
      "Epoch 102/400  loss=0.6421\n",
      "Epoch 103/400  loss=0.7519\n",
      "Epoch 104/400  loss=0.6913\n",
      "Epoch 105/400  loss=0.8658\n",
      "Epoch 106/400  loss=0.8558\n",
      "Epoch 107/400  loss=0.7270\n",
      "Epoch 108/400  loss=0.8265\n",
      "Epoch 109/400  loss=0.7167\n",
      "Epoch 110/400  loss=0.7076\n",
      "Epoch 111/400  loss=0.6652\n",
      "Epoch 112/400  loss=0.8689\n",
      "Epoch 113/400  loss=0.6966\n",
      "Epoch 114/400  loss=0.7578\n",
      "Epoch 115/400  loss=0.7675\n",
      "Epoch 116/400  loss=0.7695\n",
      "Epoch 117/400  loss=0.7601\n",
      "Epoch 118/400  loss=0.7267\n",
      "Epoch 119/400  loss=0.7763\n",
      "Epoch 120/400  loss=0.6331\n",
      "Epoch 121/400  loss=0.8358\n",
      "Epoch 122/400  loss=0.6746\n",
      "Epoch 123/400  loss=0.7131\n",
      "Epoch 124/400  loss=0.7786\n",
      "Epoch 125/400  loss=0.8203\n",
      "Epoch 126/400  loss=0.8837\n",
      "Epoch 127/400  loss=0.8651\n",
      "Epoch 128/400  loss=0.7304\n",
      "Epoch 129/400  loss=0.5739\n",
      "Epoch 130/400  loss=0.7188\n",
      "Epoch 131/400  loss=0.7281\n",
      "Epoch 132/400  loss=0.6019\n",
      "Epoch 133/400  loss=0.6814\n",
      "Epoch 134/400  loss=0.7763\n",
      "Epoch 135/400  loss=0.7727\n",
      "Epoch 136/400  loss=0.8026\n",
      "Epoch 137/400  loss=0.8082\n",
      "Epoch 138/400  loss=0.7079\n",
      "Epoch 139/400  loss=0.8902\n",
      "Epoch 140/400  loss=0.8224\n",
      "Epoch 141/400  loss=0.5859\n",
      "Epoch 142/400  loss=0.7673\n",
      "Epoch 143/400  loss=0.6241\n",
      "Epoch 144/400  loss=0.8954\n",
      "Epoch 145/400  loss=0.7343\n",
      "Epoch 146/400  loss=0.7815\n",
      "Epoch 147/400  loss=0.6648\n",
      "Epoch 148/400  loss=0.7654\n",
      "Epoch 149/400  loss=0.6381\n",
      "Epoch 150/400  loss=0.7335\n",
      "Epoch 151/400  loss=0.6097\n",
      "Epoch 152/400  loss=0.6862\n",
      "Epoch 153/400  loss=0.8613\n",
      "Epoch 154/400  loss=0.6427\n",
      "Epoch 155/400  loss=0.7858\n",
      "Epoch 156/400  loss=0.6642\n",
      "Epoch 157/400  loss=0.8800\n",
      "Epoch 158/400  loss=0.6433\n",
      "Epoch 159/400  loss=0.8777\n",
      "Epoch 160/400  loss=0.7176\n",
      "Epoch 161/400  loss=0.7918\n",
      "Epoch 162/400  loss=0.7355\n",
      "Epoch 163/400  loss=0.6864\n",
      "Epoch 164/400  loss=0.6602\n",
      "Epoch 165/400  loss=0.8606\n",
      "Epoch 166/400  loss=0.7746\n",
      "Epoch 167/400  loss=0.7274\n",
      "Epoch 168/400  loss=0.6838\n",
      "Epoch 169/400  loss=0.6332\n",
      "Epoch 170/400  loss=0.7634\n",
      "Epoch 171/400  loss=0.8787\n",
      "Epoch 172/400  loss=0.6225\n",
      "Epoch 173/400  loss=0.6643\n",
      "Epoch 174/400  loss=1.0130\n",
      "Epoch 175/400  loss=0.7576\n",
      "Epoch 176/400  loss=0.7909\n",
      "Epoch 177/400  loss=0.5987\n",
      "Epoch 178/400  loss=0.8429\n",
      "Epoch 179/400  loss=0.6584\n",
      "Epoch 180/400  loss=0.7708\n",
      "Epoch 181/400  loss=0.7374\n",
      "Epoch 182/400  loss=0.8045\n",
      "Epoch 183/400  loss=0.7732\n",
      "Epoch 184/400  loss=0.5565\n",
      "Epoch 185/400  loss=0.7469\n",
      "Epoch 186/400  loss=0.8081\n",
      "Epoch 187/400  loss=0.7392\n",
      "Epoch 188/400  loss=0.8517\n",
      "Epoch 189/400  loss=0.6977\n",
      "Epoch 190/400  loss=0.5966\n",
      "Epoch 191/400  loss=0.6809\n",
      "Epoch 192/400  loss=0.7419\n",
      "Epoch 193/400  loss=0.6410\n",
      "Epoch 194/400  loss=0.8691\n",
      "Epoch 195/400  loss=0.7168\n",
      "Epoch 196/400  loss=0.7173\n",
      "Epoch 197/400  loss=0.7309\n",
      "Epoch 198/400  loss=0.7088\n",
      "Epoch 199/400  loss=0.7651\n",
      "Epoch 200/400  loss=0.7873\n",
      "Epoch 201/400  loss=0.6712\n",
      "Epoch 202/400  loss=0.6809\n",
      "Epoch 203/400  loss=0.7292\n",
      "Epoch 204/400  loss=0.6207\n",
      "Epoch 205/400  loss=0.6363\n",
      "Epoch 206/400  loss=0.7236\n",
      "Epoch 207/400  loss=0.7801\n",
      "Epoch 208/400  loss=0.6683\n",
      "Epoch 209/400  loss=0.6497\n",
      "Epoch 210/400  loss=0.8582\n",
      "Epoch 211/400  loss=0.7119\n",
      "Epoch 212/400  loss=0.6604\n",
      "Epoch 213/400  loss=0.6086\n",
      "Epoch 214/400  loss=0.7373\n",
      "Epoch 215/400  loss=0.6637\n",
      "Epoch 216/400  loss=0.8187\n",
      "Epoch 217/400  loss=0.6880\n",
      "Epoch 218/400  loss=0.5577\n",
      "Epoch 219/400  loss=0.7309\n",
      "Epoch 220/400  loss=0.6213\n",
      "Epoch 221/400  loss=0.8159\n",
      "Epoch 222/400  loss=0.7305\n",
      "Epoch 223/400  loss=0.6568\n",
      "Epoch 224/400  loss=0.6500\n",
      "Epoch 225/400  loss=0.7324\n",
      "Epoch 226/400  loss=0.8093\n",
      "Epoch 227/400  loss=0.7205\n",
      "Epoch 228/400  loss=0.6919\n",
      "Epoch 229/400  loss=0.6967\n",
      "Epoch 230/400  loss=0.6555\n",
      "Epoch 231/400  loss=0.5844\n",
      "Epoch 232/400  loss=0.8411\n",
      "Epoch 233/400  loss=0.8102\n",
      "Epoch 234/400  loss=0.6918\n",
      "Epoch 235/400  loss=0.7014\n",
      "Epoch 236/400  loss=0.6506\n",
      "Epoch 237/400  loss=0.6650\n",
      "Epoch 238/400  loss=0.6540\n",
      "Epoch 239/400  loss=0.7833\n",
      "Epoch 240/400  loss=0.8197\n",
      "Epoch 241/400  loss=0.7007\n",
      "Epoch 242/400  loss=0.7422\n",
      "Epoch 243/400  loss=0.6226\n",
      "Epoch 244/400  loss=0.6083\n",
      "Epoch 245/400  loss=0.6070\n",
      "Epoch 246/400  loss=0.7352\n",
      "Epoch 247/400  loss=0.7387\n",
      "Epoch 248/400  loss=0.6796\n",
      "Epoch 249/400  loss=0.6778\n",
      "Epoch 250/400  loss=0.7094\n",
      "Epoch 251/400  loss=0.8650\n",
      "Epoch 252/400  loss=0.7681\n",
      "Epoch 253/400  loss=0.7025\n",
      "Epoch 254/400  loss=0.6699\n",
      "Epoch 255/400  loss=0.7263\n",
      "Epoch 256/400  loss=0.7002\n",
      "Epoch 257/400  loss=0.7426\n",
      "Epoch 258/400  loss=0.7430\n",
      "Epoch 259/400  loss=0.6335\n",
      "Epoch 260/400  loss=0.8068\n",
      "Epoch 261/400  loss=0.6507\n",
      "Epoch 262/400  loss=0.6261\n",
      "Epoch 263/400  loss=0.6743\n",
      "Epoch 264/400  loss=0.6724\n",
      "Epoch 265/400  loss=0.7054\n",
      "Epoch 266/400  loss=0.7502\n",
      "Epoch 267/400  loss=0.8047\n",
      "Epoch 268/400  loss=0.6866\n",
      "Epoch 269/400  loss=0.9115\n",
      "Epoch 270/400  loss=0.5815\n",
      "Epoch 271/400  loss=0.5710\n",
      "Epoch 272/400  loss=0.7578\n",
      "Epoch 273/400  loss=0.6837\n",
      "Epoch 274/400  loss=0.8140\n",
      "Epoch 275/400  loss=0.7442\n",
      "Epoch 276/400  loss=0.7796\n",
      "Epoch 277/400  loss=0.8705\n",
      "Epoch 278/400  loss=0.7738\n",
      "Epoch 279/400  loss=0.6321\n",
      "Epoch 280/400  loss=0.7462\n",
      "Epoch 281/400  loss=0.7309\n",
      "Epoch 282/400  loss=0.6068\n",
      "Epoch 283/400  loss=0.8573\n",
      "Epoch 284/400  loss=0.7395\n",
      "Epoch 285/400  loss=0.6624\n",
      "Epoch 286/400  loss=0.7912\n",
      "Epoch 287/400  loss=0.7278\n",
      "Epoch 288/400  loss=0.6666\n",
      "Epoch 289/400  loss=0.8675\n",
      "Epoch 290/400  loss=0.6777\n",
      "Epoch 291/400  loss=0.6658\n",
      "Epoch 292/400  loss=0.6664\n",
      "Epoch 293/400  loss=0.5158\n",
      "Epoch 294/400  loss=0.7535\n",
      "Epoch 295/400  loss=0.6236\n",
      "Epoch 296/400  loss=0.6558\n",
      "Epoch 297/400  loss=0.7080\n",
      "Epoch 298/400  loss=0.7302\n",
      "Epoch 299/400  loss=0.7377\n",
      "Epoch 300/400  loss=0.7981\n",
      "Epoch 301/400  loss=0.7213\n",
      "Epoch 302/400  loss=0.6350\n",
      "Epoch 303/400  loss=0.8452\n",
      "Epoch 304/400  loss=0.7465\n",
      "Epoch 305/400  loss=0.7772\n",
      "Epoch 306/400  loss=0.7422\n",
      "Epoch 307/400  loss=0.7394\n",
      "Epoch 308/400  loss=0.7336\n",
      "Epoch 309/400  loss=0.7527\n",
      "Epoch 310/400  loss=0.7335\n",
      "Epoch 311/400  loss=0.7723\n",
      "Epoch 312/400  loss=0.5833\n",
      "Epoch 313/400  loss=0.6632\n",
      "Epoch 314/400  loss=0.7547\n",
      "Epoch 315/400  loss=0.7090\n",
      "Epoch 316/400  loss=0.6446\n",
      "Epoch 317/400  loss=0.7596\n",
      "Epoch 318/400  loss=0.6668\n",
      "Epoch 319/400  loss=0.8978\n",
      "Epoch 320/400  loss=0.6853\n",
      "Epoch 321/400  loss=0.6294\n",
      "Epoch 322/400  loss=0.6925\n",
      "Epoch 323/400  loss=0.7604\n",
      "Epoch 324/400  loss=0.6215\n",
      "Epoch 325/400  loss=0.7288\n",
      "Epoch 326/400  loss=0.6175\n",
      "Epoch 327/400  loss=0.6977\n",
      "Epoch 328/400  loss=0.7292\n",
      "Epoch 329/400  loss=0.8083\n",
      "Epoch 330/400  loss=0.5820\n",
      "Epoch 331/400  loss=0.6380\n",
      "Epoch 332/400  loss=0.7326\n",
      "Epoch 333/400  loss=0.6504\n",
      "Epoch 334/400  loss=0.7292\n",
      "Epoch 335/400  loss=0.6363\n",
      "Epoch 336/400  loss=0.7148\n",
      "Epoch 337/400  loss=0.7613\n",
      "Epoch 338/400  loss=0.9099\n",
      "Epoch 339/400  loss=0.6501\n",
      "Epoch 340/400  loss=0.7151\n",
      "Epoch 341/400  loss=0.7237\n",
      "Epoch 342/400  loss=0.6973\n",
      "Epoch 343/400  loss=0.7253\n",
      "Epoch 344/400  loss=0.7367\n",
      "Epoch 345/400  loss=0.5648\n",
      "Epoch 346/400  loss=0.5586\n",
      "Epoch 347/400  loss=0.6008\n",
      "Epoch 348/400  loss=0.6191\n",
      "Epoch 349/400  loss=0.7193\n",
      "Epoch 350/400  loss=0.8373\n",
      "Epoch 351/400  loss=0.6208\n",
      "Epoch 352/400  loss=0.7850\n",
      "Epoch 353/400  loss=0.6957\n",
      "Epoch 354/400  loss=0.8129\n",
      "Epoch 355/400  loss=0.7558\n",
      "Epoch 356/400  loss=0.5852\n",
      "Epoch 357/400  loss=0.7472\n",
      "Epoch 358/400  loss=0.7544\n",
      "Epoch 359/400  loss=0.7905\n",
      "Epoch 360/400  loss=0.6998\n",
      "Epoch 361/400  loss=0.6259\n",
      "Epoch 362/400  loss=0.6161\n",
      "Epoch 363/400  loss=0.7872\n",
      "Epoch 364/400  loss=0.7043\n",
      "Epoch 365/400  loss=0.6479\n",
      "Epoch 366/400  loss=0.8186\n",
      "Epoch 367/400  loss=0.7900\n",
      "Epoch 368/400  loss=0.7654\n",
      "Epoch 369/400  loss=0.7122\n",
      "Epoch 370/400  loss=0.9195\n",
      "Epoch 371/400  loss=0.7205\n",
      "Epoch 372/400  loss=0.8541\n",
      "Epoch 373/400  loss=0.7062\n",
      "Epoch 374/400  loss=0.6459\n",
      "Epoch 375/400  loss=0.7081\n",
      "Epoch 376/400  loss=0.7570\n",
      "Epoch 377/400  loss=0.6304\n",
      "Epoch 378/400  loss=0.8198\n",
      "Epoch 379/400  loss=0.6401\n",
      "Epoch 380/400  loss=0.7225\n",
      "Epoch 381/400  loss=0.6475\n",
      "Epoch 382/400  loss=0.6713\n",
      "Epoch 383/400  loss=0.8374\n",
      "Epoch 384/400  loss=0.6917\n",
      "Epoch 385/400  loss=0.7579\n",
      "Epoch 386/400  loss=0.5792\n",
      "Epoch 387/400  loss=0.5259\n",
      "Epoch 388/400  loss=0.6517\n",
      "Epoch 389/400  loss=0.5807\n",
      "Epoch 390/400  loss=0.8268\n",
      "Epoch 391/400  loss=0.6149\n",
      "Epoch 392/400  loss=0.7405\n",
      "Epoch 393/400  loss=0.7298\n",
      "Epoch 394/400  loss=0.6358\n",
      "Epoch 395/400  loss=0.7582\n",
      "Epoch 396/400  loss=0.6181\n",
      "Epoch 397/400  loss=0.7753\n",
      "Epoch 398/400  loss=0.7314\n",
      "Epoch 399/400  loss=0.8155\n",
      "Epoch 400/400  loss=0.7485\n",
      "Saved generated trajectories: (200, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Imports & Config\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import torch, math, random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "K = 20\n",
    "input_path   = f'traj_{K}.npy'\n",
    "output_path  = f'traj_{K}_generated_seq_naive.npy'\n",
    "BATCH_SIZE   = 10\n",
    "EPOCHS       = 400\n",
    "T_STEPS      = 50          # diffusion steps\n",
    "LR           = 2e-4\n",
    "EMBED_DIM    = 64\n",
    "N_LAYERS     = 4\n",
    "N_HEADS      = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_R   = 2\n",
    "NUM_TOK = K * NUM_R        # 每个 time-step 的组合 token 种数\n",
    "# =========================================================\n",
    "# 2. Data : (N, L, 2) → (N, L) integer tokens\n",
    "# =========================================================\n",
    "raw = np.load(input_path)               # (N, L, 2)\n",
    "SEQ_LEN = raw.shape[1]\n",
    "print('Loaded', raw.shape[0], 'trajectories of length', SEQ_LEN)\n",
    "a_arr = raw[:, :, 0].astype(np.int64)\n",
    "r_arr = raw[:, :, 1].astype(np.int64)\n",
    "tok_arr = (a_arr * NUM_R + r_arr).astype(np.int64)   # (N, L)\n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tok = torch.from_numpy(tokens).long()\n",
    "    def __len__(self):\n",
    "        return self.tok.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tok[idx]          # (L,)\n",
    "dataloader = DataLoader(TrajDataset(tok_arr), batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=True)\n",
    "# =========================================================\n",
    "# 3. Discrete forward diffusion helper (token → token)\n",
    "# =========================================================\n",
    "def forward_diffusion(x0, betas, num_classes):\n",
    "    \"\"\"x0 : (B, L) LongTensor → list len T+1, each (B, L)\"\"\"\n",
    "    traj = [x0]\n",
    "    x_prev = x0\n",
    "    for beta in betas:\n",
    "        mask  = (torch.rand_like(x_prev.float()) < beta)\n",
    "        noise = torch.randint(0, num_classes, x_prev.shape, device=x_prev.device)\n",
    "        x_next = torch.where(mask, noise, x_prev)\n",
    "        traj.append(x_next)\n",
    "        x_prev = x_next\n",
    "    return traj\n",
    "# =========================================================\n",
    "# 4. Model : sequence-agnostic baseline (no Transformer / RNN)\n",
    "# =========================================================\n",
    "class SeqDiscreteDiffusion(nn.Module):\n",
    "    def __init__(self, num_tok=NUM_TOK, seq_len=SEQ_LEN, embed_dim=EMBED_DIM, n_layers=N_LAYERS, n_heads=None):\n",
    "        super().__init__()\n",
    "        # token / position / diffusion-step embeddings\n",
    "        self.token_emb = nn.Embedding(num_tok, embed_dim)\n",
    "        self.pos_emb   = nn.Embedding(seq_len, embed_dim)\n",
    "        self.time_emb  = nn.Embedding(1000, embed_dim)\n",
    "\n",
    "        # simple token-wise MLP (shared across all positions)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "        # projection to logits\n",
    "        self.head = nn.Linear(embed_dim, num_tok)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t : (B, L)  LongTensor   noisy tokens at step t\n",
    "        t   : (B,)    LongTensor   diffusion step index for each sample\n",
    "        \"\"\"\n",
    "        B, L = x_t.shape\n",
    "\n",
    "        # embeddings\n",
    "        tok  = self.token_emb(x_t)                           # (B,L,E)\n",
    "        pos  = self.pos_emb(torch.arange(L, device=x_t.device)).unsqueeze(0).expand(B, -1, -1)           # (B,L,E)\n",
    "        time = self.time_emb(t).unsqueeze(1).expand(-1, L, -1)# (B,L,E)\n",
    "\n",
    "        h = tok + pos + time                                # (B,L,E)\n",
    "\n",
    "        # apply MLP independently on every position\n",
    "        h = self.mlp(h)                                     # (B,L,E)\n",
    "\n",
    "        logits = self.head(h)                               # (B,L,NUM_TOK)\n",
    "        return logits\n",
    "# =========================================================\n",
    "# 5. Training loop\n",
    "# =========================================================\n",
    "betas = [0.1] * T_STEPS\n",
    "model = SeqDiscreteDiffusion().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for x0 in dataloader:          # x0 : (B,L)\n",
    "        x0 = x0.to(device)\n",
    "        B, L = x0.shape\n",
    "        # forward diffusion once per batch (global t for the whole seq)\n",
    "        traj  = forward_diffusion(x0, betas, NUM_TOK)      # list len T+1\n",
    "        t_bar = torch.randint(1, T_STEPS + 1, (B,), device=device)  # (B,)\n",
    "        x_t   = torch.stack([traj[t][i]   for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        x_prev= torch.stack([traj[t-1][i] for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        logits = model(x_t, t_bar)                         # (B,L,NUM_TOK)\n",
    "        loss   = ce(logits.reshape(-1, NUM_TOK), x_prev.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}  loss={loss.item():.4f}')\n",
    "# =========================================================\n",
    "# 6. Sampling (reverse diffusion)\n",
    "# =========================================================\n",
    "NUM_SAMPLES = 100   # 可自行调整\n",
    "def sample(model, n_samples=NUM_SAMPLES):\n",
    "    model.eval()\n",
    "    B = n_samples\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randint(0, NUM_TOK, (B, SEQ_LEN), device=device)\n",
    "        for t in reversed(range(1, T_STEPS + 1)):\n",
    "            t_vec = torch.full((B,), t, device=device)\n",
    "            logits = model(x_t, t_vec)                     # (B,L,NUM_TOK)\n",
    "            probs  = torch.softmax(logits, dim=-1)\n",
    "            x_t    = torch.multinomial(probs.view(-1, NUM_TOK), 1).squeeze(-1).view(B, SEQ_LEN)      # (B,L)\n",
    "        return x_t.cpu()                                   # (B,L)\n",
    "gen_tok = sample(model, NUM_SAMPLES)                       # (N,L)\n",
    "# 解码回 (a,r) 二元组\n",
    "gen_a = (gen_tok // NUM_R).numpy()\n",
    "gen_r = (gen_tok %  NUM_R).numpy()\n",
    "gen_traj = np.stack([gen_a, gen_r], axis=-1)               # (N,L,2)\n",
    "# concat the training data `raw`` and the generated `data gen_traj`\n",
    "trajs = np.concatenate([raw, gen_traj], axis=0)          # (N+200, L, 2)\n",
    "np.save(output_path, trajs)\n",
    "print('Saved generated trajectories:', trajs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
