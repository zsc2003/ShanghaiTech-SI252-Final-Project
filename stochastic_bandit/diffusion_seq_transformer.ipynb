{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 trajectories of length 50\n",
      "Epoch 1/400  loss=3.6359\n",
      "Epoch 2/400  loss=3.5793\n",
      "Epoch 3/400  loss=3.4527\n",
      "Epoch 4/400  loss=3.2046\n",
      "Epoch 5/400  loss=2.9985\n",
      "Epoch 6/400  loss=2.8418\n",
      "Epoch 7/400  loss=2.6438\n",
      "Epoch 8/400  loss=2.3934\n",
      "Epoch 9/400  loss=2.3006\n",
      "Epoch 10/400  loss=2.1280\n",
      "Epoch 11/400  loss=2.0386\n",
      "Epoch 12/400  loss=1.8051\n",
      "Epoch 13/400  loss=1.7322\n",
      "Epoch 14/400  loss=1.6306\n",
      "Epoch 15/400  loss=1.5147\n",
      "Epoch 16/400  loss=1.4781\n",
      "Epoch 17/400  loss=1.2538\n",
      "Epoch 18/400  loss=1.2994\n",
      "Epoch 19/400  loss=1.1750\n",
      "Epoch 20/400  loss=1.1031\n",
      "Epoch 21/400  loss=1.1033\n",
      "Epoch 22/400  loss=1.0178\n",
      "Epoch 23/400  loss=1.1229\n",
      "Epoch 24/400  loss=0.9111\n",
      "Epoch 25/400  loss=0.8951\n",
      "Epoch 26/400  loss=0.9279\n",
      "Epoch 27/400  loss=0.8683\n",
      "Epoch 28/400  loss=0.9636\n",
      "Epoch 29/400  loss=0.8921\n",
      "Epoch 30/400  loss=0.8160\n",
      "Epoch 31/400  loss=0.8542\n",
      "Epoch 32/400  loss=0.9601\n",
      "Epoch 33/400  loss=0.8276\n",
      "Epoch 34/400  loss=0.7785\n",
      "Epoch 35/400  loss=0.7825\n",
      "Epoch 36/400  loss=0.8422\n",
      "Epoch 37/400  loss=0.7262\n",
      "Epoch 38/400  loss=0.8282\n",
      "Epoch 39/400  loss=0.8574\n",
      "Epoch 40/400  loss=0.7046\n",
      "Epoch 41/400  loss=0.7581\n",
      "Epoch 42/400  loss=0.7020\n",
      "Epoch 43/400  loss=0.8457\n",
      "Epoch 44/400  loss=0.6691\n",
      "Epoch 45/400  loss=0.7918\n",
      "Epoch 46/400  loss=0.7434\n",
      "Epoch 47/400  loss=0.7590\n",
      "Epoch 48/400  loss=0.7259\n",
      "Epoch 49/400  loss=0.8165\n",
      "Epoch 50/400  loss=0.7825\n",
      "Epoch 51/400  loss=0.7288\n",
      "Epoch 52/400  loss=0.6850\n",
      "Epoch 53/400  loss=0.7097\n",
      "Epoch 54/400  loss=0.7867\n",
      "Epoch 55/400  loss=0.6074\n",
      "Epoch 56/400  loss=0.7417\n",
      "Epoch 57/400  loss=0.6666\n",
      "Epoch 58/400  loss=0.6864\n",
      "Epoch 59/400  loss=0.6393\n",
      "Epoch 60/400  loss=0.5761\n",
      "Epoch 61/400  loss=0.8595\n",
      "Epoch 62/400  loss=0.7178\n",
      "Epoch 63/400  loss=0.7295\n",
      "Epoch 64/400  loss=0.6867\n",
      "Epoch 65/400  loss=0.9380\n",
      "Epoch 66/400  loss=0.7121\n",
      "Epoch 67/400  loss=0.7813\n",
      "Epoch 68/400  loss=0.7126\n",
      "Epoch 69/400  loss=0.7964\n",
      "Epoch 70/400  loss=0.7328\n",
      "Epoch 71/400  loss=0.7495\n",
      "Epoch 72/400  loss=0.8183\n",
      "Epoch 73/400  loss=0.7051\n",
      "Epoch 74/400  loss=0.8434\n",
      "Epoch 75/400  loss=0.6339\n",
      "Epoch 76/400  loss=0.7353\n",
      "Epoch 77/400  loss=0.6489\n",
      "Epoch 78/400  loss=0.6668\n",
      "Epoch 79/400  loss=0.7428\n",
      "Epoch 80/400  loss=0.6339\n",
      "Epoch 81/400  loss=0.6765\n",
      "Epoch 82/400  loss=0.7465\n",
      "Epoch 83/400  loss=0.7450\n",
      "Epoch 84/400  loss=0.6999\n",
      "Epoch 85/400  loss=0.7210\n",
      "Epoch 86/400  loss=0.7616\n",
      "Epoch 87/400  loss=0.6196\n",
      "Epoch 88/400  loss=0.8303\n",
      "Epoch 89/400  loss=0.6967\n",
      "Epoch 90/400  loss=0.7017\n",
      "Epoch 91/400  loss=0.6550\n",
      "Epoch 92/400  loss=0.7128\n",
      "Epoch 93/400  loss=0.7794\n",
      "Epoch 94/400  loss=0.7738\n",
      "Epoch 95/400  loss=0.6919\n",
      "Epoch 96/400  loss=0.8689\n",
      "Epoch 97/400  loss=0.5663\n",
      "Epoch 98/400  loss=0.7007\n",
      "Epoch 99/400  loss=0.7653\n",
      "Epoch 100/400  loss=0.7479\n",
      "Epoch 101/400  loss=0.7459\n",
      "Epoch 102/400  loss=0.6819\n",
      "Epoch 103/400  loss=0.6387\n",
      "Epoch 104/400  loss=0.6173\n",
      "Epoch 105/400  loss=0.6180\n",
      "Epoch 106/400  loss=0.7938\n",
      "Epoch 107/400  loss=0.5295\n",
      "Epoch 108/400  loss=0.6070\n",
      "Epoch 109/400  loss=0.7482\n",
      "Epoch 110/400  loss=0.7028\n",
      "Epoch 111/400  loss=0.6819\n",
      "Epoch 112/400  loss=0.7142\n",
      "Epoch 113/400  loss=0.7236\n",
      "Epoch 114/400  loss=0.7357\n",
      "Epoch 115/400  loss=0.5934\n",
      "Epoch 116/400  loss=0.6626\n",
      "Epoch 117/400  loss=0.6726\n",
      "Epoch 118/400  loss=0.7509\n",
      "Epoch 119/400  loss=0.7332\n",
      "Epoch 120/400  loss=0.7097\n",
      "Epoch 121/400  loss=0.6742\n",
      "Epoch 122/400  loss=0.7358\n",
      "Epoch 123/400  loss=0.7493\n",
      "Epoch 124/400  loss=0.6169\n",
      "Epoch 125/400  loss=0.6479\n",
      "Epoch 126/400  loss=0.7083\n",
      "Epoch 127/400  loss=0.7693\n",
      "Epoch 128/400  loss=0.7452\n",
      "Epoch 129/400  loss=0.5594\n",
      "Epoch 130/400  loss=0.8593\n",
      "Epoch 131/400  loss=0.9148\n",
      "Epoch 132/400  loss=0.7556\n",
      "Epoch 133/400  loss=0.7005\n",
      "Epoch 134/400  loss=0.8557\n",
      "Epoch 135/400  loss=0.6108\n",
      "Epoch 136/400  loss=0.8332\n",
      "Epoch 137/400  loss=0.7127\n",
      "Epoch 138/400  loss=0.7918\n",
      "Epoch 139/400  loss=0.7263\n",
      "Epoch 140/400  loss=0.5781\n",
      "Epoch 141/400  loss=0.7739\n",
      "Epoch 142/400  loss=0.6672\n",
      "Epoch 143/400  loss=0.6742\n",
      "Epoch 144/400  loss=0.6177\n",
      "Epoch 145/400  loss=0.6343\n",
      "Epoch 146/400  loss=0.8002\n",
      "Epoch 147/400  loss=0.7867\n",
      "Epoch 148/400  loss=0.6605\n",
      "Epoch 149/400  loss=0.5881\n",
      "Epoch 150/400  loss=0.7165\n",
      "Epoch 151/400  loss=0.6719\n",
      "Epoch 152/400  loss=0.5784\n",
      "Epoch 153/400  loss=0.5742\n",
      "Epoch 154/400  loss=0.7450\n",
      "Epoch 155/400  loss=0.5143\n",
      "Epoch 156/400  loss=0.6498\n",
      "Epoch 157/400  loss=0.7127\n",
      "Epoch 158/400  loss=0.8010\n",
      "Epoch 159/400  loss=0.6570\n",
      "Epoch 160/400  loss=0.7364\n",
      "Epoch 161/400  loss=0.7655\n",
      "Epoch 162/400  loss=0.7916\n",
      "Epoch 163/400  loss=0.7073\n",
      "Epoch 164/400  loss=0.6051\n",
      "Epoch 165/400  loss=0.6953\n",
      "Epoch 166/400  loss=0.7898\n",
      "Epoch 167/400  loss=0.6869\n",
      "Epoch 168/400  loss=0.7382\n",
      "Epoch 169/400  loss=0.7038\n",
      "Epoch 170/400  loss=0.6953\n",
      "Epoch 171/400  loss=0.8167\n",
      "Epoch 172/400  loss=0.5824\n",
      "Epoch 173/400  loss=0.8013\n",
      "Epoch 174/400  loss=0.5535\n",
      "Epoch 175/400  loss=0.7267\n",
      "Epoch 176/400  loss=0.7940\n",
      "Epoch 177/400  loss=0.8121\n",
      "Epoch 178/400  loss=0.5531\n",
      "Epoch 179/400  loss=0.6919\n",
      "Epoch 180/400  loss=0.7163\n",
      "Epoch 181/400  loss=0.7730\n",
      "Epoch 182/400  loss=0.7320\n",
      "Epoch 183/400  loss=0.6902\n",
      "Epoch 184/400  loss=0.6968\n",
      "Epoch 185/400  loss=0.8263\n",
      "Epoch 186/400  loss=0.7609\n",
      "Epoch 187/400  loss=0.6297\n",
      "Epoch 188/400  loss=0.7077\n",
      "Epoch 189/400  loss=0.6085\n",
      "Epoch 190/400  loss=0.6086\n",
      "Epoch 191/400  loss=0.6870\n",
      "Epoch 192/400  loss=0.5685\n",
      "Epoch 193/400  loss=0.7657\n",
      "Epoch 194/400  loss=0.7298\n",
      "Epoch 195/400  loss=0.6355\n",
      "Epoch 196/400  loss=0.5505\n",
      "Epoch 197/400  loss=0.5979\n",
      "Epoch 198/400  loss=0.7808\n",
      "Epoch 199/400  loss=0.7738\n",
      "Epoch 200/400  loss=0.7641\n",
      "Epoch 201/400  loss=0.7055\n",
      "Epoch 202/400  loss=0.6207\n",
      "Epoch 203/400  loss=0.7233\n",
      "Epoch 204/400  loss=0.6002\n",
      "Epoch 205/400  loss=0.7621\n",
      "Epoch 206/400  loss=0.8460\n",
      "Epoch 207/400  loss=0.6491\n",
      "Epoch 208/400  loss=0.7844\n",
      "Epoch 209/400  loss=0.8100\n",
      "Epoch 210/400  loss=0.7178\n",
      "Epoch 211/400  loss=0.6943\n",
      "Epoch 212/400  loss=0.7258\n",
      "Epoch 213/400  loss=0.7945\n",
      "Epoch 214/400  loss=0.5163\n",
      "Epoch 215/400  loss=0.7284\n",
      "Epoch 216/400  loss=0.6193\n",
      "Epoch 217/400  loss=0.6874\n",
      "Epoch 218/400  loss=0.7461\n",
      "Epoch 219/400  loss=0.7556\n",
      "Epoch 220/400  loss=0.6995\n",
      "Epoch 221/400  loss=0.6017\n",
      "Epoch 222/400  loss=0.7224\n",
      "Epoch 223/400  loss=0.7515\n",
      "Epoch 224/400  loss=0.8331\n",
      "Epoch 225/400  loss=0.6206\n",
      "Epoch 226/400  loss=0.7397\n",
      "Epoch 227/400  loss=0.8652\n",
      "Epoch 228/400  loss=0.7895\n",
      "Epoch 229/400  loss=0.6862\n",
      "Epoch 230/400  loss=0.8074\n",
      "Epoch 231/400  loss=0.6517\n",
      "Epoch 232/400  loss=0.7936\n",
      "Epoch 233/400  loss=0.7287\n",
      "Epoch 234/400  loss=0.7840\n",
      "Epoch 235/400  loss=0.6665\n",
      "Epoch 236/400  loss=0.7805\n",
      "Epoch 237/400  loss=0.6886\n",
      "Epoch 238/400  loss=0.7208\n",
      "Epoch 239/400  loss=0.5548\n",
      "Epoch 240/400  loss=0.6716\n",
      "Epoch 241/400  loss=0.6776\n",
      "Epoch 242/400  loss=0.6608\n",
      "Epoch 243/400  loss=0.6925\n",
      "Epoch 244/400  loss=0.6294\n",
      "Epoch 245/400  loss=0.6552\n",
      "Epoch 246/400  loss=0.8346\n",
      "Epoch 247/400  loss=0.5628\n",
      "Epoch 248/400  loss=0.6594\n",
      "Epoch 249/400  loss=0.6383\n",
      "Epoch 250/400  loss=0.7056\n",
      "Epoch 251/400  loss=0.5687\n",
      "Epoch 252/400  loss=0.7806\n",
      "Epoch 253/400  loss=0.7819\n",
      "Epoch 254/400  loss=0.7393\n",
      "Epoch 255/400  loss=0.7815\n",
      "Epoch 256/400  loss=0.6234\n",
      "Epoch 257/400  loss=0.8482\n",
      "Epoch 258/400  loss=0.7685\n",
      "Epoch 259/400  loss=0.7611\n",
      "Epoch 260/400  loss=0.6959\n",
      "Epoch 261/400  loss=0.7772\n",
      "Epoch 262/400  loss=0.6598\n",
      "Epoch 263/400  loss=0.6746\n",
      "Epoch 264/400  loss=0.7529\n",
      "Epoch 265/400  loss=0.7434\n",
      "Epoch 266/400  loss=0.6240\n",
      "Epoch 267/400  loss=0.7160\n",
      "Epoch 268/400  loss=0.6364\n",
      "Epoch 269/400  loss=0.5993\n",
      "Epoch 270/400  loss=0.7129\n",
      "Epoch 271/400  loss=0.7962\n",
      "Epoch 272/400  loss=0.8100\n",
      "Epoch 273/400  loss=0.7181\n",
      "Epoch 274/400  loss=0.6644\n",
      "Epoch 275/400  loss=0.6893\n",
      "Epoch 276/400  loss=0.6316\n",
      "Epoch 277/400  loss=0.8541\n",
      "Epoch 278/400  loss=0.7363\n",
      "Epoch 279/400  loss=0.6790\n",
      "Epoch 280/400  loss=0.6005\n",
      "Epoch 281/400  loss=0.6167\n",
      "Epoch 282/400  loss=0.6916\n",
      "Epoch 283/400  loss=0.4820\n",
      "Epoch 284/400  loss=0.7791\n",
      "Epoch 285/400  loss=0.6248\n",
      "Epoch 286/400  loss=0.6471\n",
      "Epoch 287/400  loss=0.7100\n",
      "Epoch 288/400  loss=0.7636\n",
      "Epoch 289/400  loss=0.7519\n",
      "Epoch 290/400  loss=0.6088\n",
      "Epoch 291/400  loss=0.7368\n",
      "Epoch 292/400  loss=0.7935\n",
      "Epoch 293/400  loss=0.7145\n",
      "Epoch 294/400  loss=0.7529\n",
      "Epoch 295/400  loss=0.8457\n",
      "Epoch 296/400  loss=0.6648\n",
      "Epoch 297/400  loss=0.6204\n",
      "Epoch 298/400  loss=0.5030\n",
      "Epoch 299/400  loss=0.6359\n",
      "Epoch 300/400  loss=0.5558\n",
      "Epoch 301/400  loss=0.7071\n",
      "Epoch 302/400  loss=0.8273\n",
      "Epoch 303/400  loss=0.7154\n",
      "Epoch 304/400  loss=0.6753\n",
      "Epoch 305/400  loss=0.6338\n",
      "Epoch 306/400  loss=0.9588\n",
      "Epoch 307/400  loss=0.6450\n",
      "Epoch 308/400  loss=0.6614\n",
      "Epoch 309/400  loss=0.7057\n",
      "Epoch 310/400  loss=0.6170\n",
      "Epoch 311/400  loss=0.7074\n",
      "Epoch 312/400  loss=0.7496\n",
      "Epoch 313/400  loss=0.6507\n",
      "Epoch 314/400  loss=0.7407\n",
      "Epoch 315/400  loss=0.7029\n",
      "Epoch 316/400  loss=0.6233\n",
      "Epoch 317/400  loss=0.7459\n",
      "Epoch 318/400  loss=0.6151\n",
      "Epoch 319/400  loss=0.8362\n",
      "Epoch 320/400  loss=0.5681\n",
      "Epoch 321/400  loss=0.7612\n",
      "Epoch 322/400  loss=0.7280\n",
      "Epoch 323/400  loss=0.6053\n",
      "Epoch 324/400  loss=0.8692\n",
      "Epoch 325/400  loss=0.8783\n",
      "Epoch 326/400  loss=0.6858\n",
      "Epoch 327/400  loss=0.8737\n",
      "Epoch 328/400  loss=0.7060\n",
      "Epoch 329/400  loss=0.7286\n",
      "Epoch 330/400  loss=0.6829\n",
      "Epoch 331/400  loss=0.6375\n",
      "Epoch 332/400  loss=0.8228\n",
      "Epoch 333/400  loss=0.5827\n",
      "Epoch 334/400  loss=0.6183\n",
      "Epoch 335/400  loss=0.6485\n",
      "Epoch 336/400  loss=0.7419\n",
      "Epoch 337/400  loss=0.6487\n",
      "Epoch 338/400  loss=0.6466\n",
      "Epoch 339/400  loss=0.6825\n",
      "Epoch 340/400  loss=0.7025\n",
      "Epoch 341/400  loss=0.7655\n",
      "Epoch 342/400  loss=0.5898\n",
      "Epoch 343/400  loss=0.6049\n",
      "Epoch 344/400  loss=0.6506\n",
      "Epoch 345/400  loss=0.6839\n",
      "Epoch 346/400  loss=0.7815\n",
      "Epoch 347/400  loss=0.8919\n",
      "Epoch 348/400  loss=0.6843\n",
      "Epoch 349/400  loss=0.6496\n",
      "Epoch 350/400  loss=0.7430\n",
      "Epoch 351/400  loss=0.6537\n",
      "Epoch 352/400  loss=0.7320\n",
      "Epoch 353/400  loss=0.6371\n",
      "Epoch 354/400  loss=0.6108\n",
      "Epoch 355/400  loss=0.7427\n",
      "Epoch 356/400  loss=0.7082\n",
      "Epoch 357/400  loss=0.7671\n",
      "Epoch 358/400  loss=0.7931\n",
      "Epoch 359/400  loss=0.6952\n",
      "Epoch 360/400  loss=0.7252\n",
      "Epoch 361/400  loss=0.7308\n",
      "Epoch 362/400  loss=0.7237\n",
      "Epoch 363/400  loss=0.7618\n",
      "Epoch 364/400  loss=0.6728\n",
      "Epoch 365/400  loss=0.6135\n",
      "Epoch 366/400  loss=0.6222\n",
      "Epoch 367/400  loss=0.5656\n",
      "Epoch 368/400  loss=0.6991\n",
      "Epoch 369/400  loss=0.7647\n",
      "Epoch 370/400  loss=0.6050\n",
      "Epoch 371/400  loss=0.6524\n",
      "Epoch 372/400  loss=0.7535\n",
      "Epoch 373/400  loss=0.7664\n",
      "Epoch 374/400  loss=0.6614\n",
      "Epoch 375/400  loss=0.6322\n",
      "Epoch 376/400  loss=0.5784\n",
      "Epoch 377/400  loss=0.5490\n",
      "Epoch 378/400  loss=0.6096\n",
      "Epoch 379/400  loss=0.6138\n",
      "Epoch 380/400  loss=0.7329\n",
      "Epoch 381/400  loss=0.8243\n",
      "Epoch 382/400  loss=0.5916\n",
      "Epoch 383/400  loss=0.7445\n",
      "Epoch 384/400  loss=0.7085\n",
      "Epoch 385/400  loss=0.6836\n",
      "Epoch 386/400  loss=0.7596\n",
      "Epoch 387/400  loss=0.6589\n",
      "Epoch 388/400  loss=0.6939\n",
      "Epoch 389/400  loss=0.6869\n",
      "Epoch 390/400  loss=0.6044\n",
      "Epoch 391/400  loss=0.6651\n",
      "Epoch 392/400  loss=0.7522\n",
      "Epoch 393/400  loss=0.5956\n",
      "Epoch 394/400  loss=0.6661\n",
      "Epoch 395/400  loss=0.6779\n",
      "Epoch 396/400  loss=0.6155\n",
      "Epoch 397/400  loss=0.7220\n",
      "Epoch 398/400  loss=0.7699\n",
      "Epoch 399/400  loss=0.5641\n",
      "Epoch 400/400  loss=0.7683\n",
      "Saved generated trajectories: (600, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Imports & Config\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import torch, math, random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "K = 20\n",
    "NUM_SAMPLES = 500\n",
    "input_path   = f'traj_{K}.npy'\n",
    "output_path  = f'traj_{K}_generated_seq_transformer_{NUM_SAMPLES}.npy'\n",
    "\n",
    "\n",
    "BATCH_SIZE   = 10\n",
    "EPOCHS       = 400\n",
    "T_STEPS      = 50          # diffusion steps\n",
    "LR           = 2e-4\n",
    "EMBED_DIM    = 64\n",
    "N_LAYERS     = 4\n",
    "N_HEADS      = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_R   = 2\n",
    "NUM_TOK = K * NUM_R        # 每个 time-step 的组合 token 种数\n",
    "# =========================================================\n",
    "# 2. Data : (N, L, 2) → (N, L) integer tokens\n",
    "# =========================================================\n",
    "raw = np.load(input_path)               # (N, L, 2)\n",
    "SEQ_LEN = raw.shape[1]\n",
    "print('Loaded', raw.shape[0], 'trajectories of length', SEQ_LEN)\n",
    "a_arr = raw[:, :, 0].astype(np.int64)\n",
    "r_arr = raw[:, :, 1].astype(np.int64)\n",
    "tok_arr = (a_arr * NUM_R + r_arr).astype(np.int64)   # (N, L)\n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tok = torch.from_numpy(tokens).long()\n",
    "    def __len__(self):\n",
    "        return self.tok.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tok[idx]          # (L,)\n",
    "dataloader = DataLoader(TrajDataset(tok_arr), batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=True)\n",
    "# =========================================================\n",
    "# 3. Discrete forward diffusion helper (token → token)\n",
    "# =========================================================\n",
    "def forward_diffusion(x0, betas, num_classes):\n",
    "    \"\"\"x0 : (B, L) LongTensor → list len T+1, each (B, L)\"\"\"\n",
    "    traj = [x0]\n",
    "    x_prev = x0\n",
    "    for beta in betas:\n",
    "        mask  = (torch.rand_like(x_prev.float()) < beta)\n",
    "        noise = torch.randint(0, num_classes, x_prev.shape, device=x_prev.device)\n",
    "        x_next = torch.where(mask, noise, x_prev)\n",
    "        traj.append(x_next)\n",
    "        x_prev = x_next\n",
    "    return traj\n",
    "# =========================================================\n",
    "# 4. Model : sequence‐aware discrete diffusion network\n",
    "# =========================================================\n",
    "class SeqDiscreteDiffusion(nn.Module):\n",
    "    def __init__(self, num_tok=NUM_TOK, seq_len=SEQ_LEN, embed_dim=EMBED_DIM, n_layers=N_LAYERS, n_heads=N_HEADS):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_tok, embed_dim)\n",
    "        self.pos_emb   = nn.Embedding(seq_len, embed_dim)\n",
    "        self.time_emb  = nn.Embedding(1000, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim*4, activation='relu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(embed_dim, num_tok)\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t : (B, L)  LongTensor\n",
    "        t   : (B,)    LongTensor  (same t for the whole seq)\n",
    "        \"\"\"\n",
    "        B, L = x_t.shape\n",
    "        tok  = self.token_emb(x_t)                         # (B,L,E)\n",
    "        pos  = self.pos_emb(torch.arange(L, device=x_t.device))  # (L,E)\n",
    "        pos  = pos.unsqueeze(0).expand(B, -1, -1)          # (B,L,E)\n",
    "        time = self.time_emb(t).unsqueeze(1).expand(-1, L, -1)   # (B,L,E)\n",
    "        h = tok + pos + time                               # (B,L,E)\n",
    "        h = self.transformer(h)                            # (B,L,E)\n",
    "        logits = self.head(h)                              # (B,L,NUM_TOK)\n",
    "        return logits\n",
    "# =========================================================\n",
    "# 5. Training loop\n",
    "# =========================================================\n",
    "betas = [0.1] * T_STEPS\n",
    "model = SeqDiscreteDiffusion().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for x0 in dataloader:          # x0 : (B,L)\n",
    "        x0 = x0.to(device)\n",
    "        B, L = x0.shape\n",
    "        # forward diffusion once per batch (global t for the whole seq)\n",
    "        traj  = forward_diffusion(x0, betas, NUM_TOK)      # list len T+1\n",
    "        t_bar = torch.randint(1, T_STEPS + 1, (B,), device=device)  # (B,)\n",
    "        x_t   = torch.stack([traj[t][i]   for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        x_prev= torch.stack([traj[t-1][i] for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        logits = model(x_t, t_bar)                         # (B,L,NUM_TOK)\n",
    "        loss   = ce(logits.reshape(-1, NUM_TOK), x_prev.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}  loss={loss.item():.4f}')\n",
    "# =========================================================\n",
    "# 6. Sampling (reverse diffusion)\n",
    "# =========================================================\n",
    "def sample(model, n_samples=NUM_SAMPLES):\n",
    "    model.eval()\n",
    "    B = n_samples\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randint(0, NUM_TOK, (B, SEQ_LEN), device=device)\n",
    "        for t in reversed(range(1, T_STEPS + 1)):\n",
    "            t_vec = torch.full((B,), t, device=device)\n",
    "            logits = model(x_t, t_vec)                     # (B,L,NUM_TOK)\n",
    "            probs  = torch.softmax(logits, dim=-1)\n",
    "            x_t    = torch.multinomial(probs.view(-1, NUM_TOK), 1).squeeze(-1).view(B, SEQ_LEN)      # (B,L)\n",
    "        return x_t.cpu()                                   # (B,L)\n",
    "gen_tok = sample(model, NUM_SAMPLES)                       # (N,L)\n",
    "# 解码回 (a,r) 二元组\n",
    "gen_a = (gen_tok // NUM_R).numpy()\n",
    "gen_r = (gen_tok %  NUM_R).numpy()\n",
    "gen_traj = np.stack([gen_a, gen_r], axis=-1)               # (N,L,2)\n",
    "# concat the training data `raw`` and the generated `data gen_traj`\n",
    "trajs = np.concatenate([raw, gen_traj], axis=0)          # (N+200, L, 2)\n",
    "np.save(output_path, trajs)\n",
    "print('Saved generated trajectories:', trajs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
