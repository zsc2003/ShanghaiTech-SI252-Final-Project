{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 trajectories of length 50\n",
      "Epoch 1/400  loss=3.7122\n",
      "Epoch 2/400  loss=3.5170\n",
      "Epoch 3/400  loss=3.3124\n",
      "Epoch 4/400  loss=3.1706\n",
      "Epoch 5/400  loss=3.0624\n",
      "Epoch 6/400  loss=2.9313\n",
      "Epoch 7/400  loss=2.6812\n",
      "Epoch 8/400  loss=2.4647\n",
      "Epoch 9/400  loss=2.2802\n",
      "Epoch 10/400  loss=2.1173\n",
      "Epoch 11/400  loss=1.9006\n",
      "Epoch 12/400  loss=1.8155\n",
      "Epoch 13/400  loss=1.6987\n",
      "Epoch 14/400  loss=1.5736\n",
      "Epoch 15/400  loss=1.5050\n",
      "Epoch 16/400  loss=1.3765\n",
      "Epoch 17/400  loss=1.1983\n",
      "Epoch 18/400  loss=1.2376\n",
      "Epoch 19/400  loss=1.1423\n",
      "Epoch 20/400  loss=1.1418\n",
      "Epoch 21/400  loss=0.9692\n",
      "Epoch 22/400  loss=0.9411\n",
      "Epoch 23/400  loss=1.0018\n",
      "Epoch 24/400  loss=0.9949\n",
      "Epoch 25/400  loss=0.9223\n",
      "Epoch 26/400  loss=0.8308\n",
      "Epoch 27/400  loss=0.8692\n",
      "Epoch 28/400  loss=0.9680\n",
      "Epoch 29/400  loss=0.8575\n",
      "Epoch 30/400  loss=0.8404\n",
      "Epoch 31/400  loss=0.8392\n",
      "Epoch 32/400  loss=0.8902\n",
      "Epoch 33/400  loss=0.8407\n",
      "Epoch 34/400  loss=0.8412\n",
      "Epoch 35/400  loss=0.7769\n",
      "Epoch 36/400  loss=0.8063\n",
      "Epoch 37/400  loss=0.7505\n",
      "Epoch 38/400  loss=0.8084\n",
      "Epoch 39/400  loss=0.8789\n",
      "Epoch 40/400  loss=0.7502\n",
      "Epoch 41/400  loss=0.6366\n",
      "Epoch 42/400  loss=0.7453\n",
      "Epoch 43/400  loss=0.6766\n",
      "Epoch 44/400  loss=0.8733\n",
      "Epoch 45/400  loss=0.6965\n",
      "Epoch 46/400  loss=0.9042\n",
      "Epoch 47/400  loss=0.7704\n",
      "Epoch 48/400  loss=0.8361\n",
      "Epoch 49/400  loss=0.8312\n",
      "Epoch 50/400  loss=0.7848\n",
      "Epoch 51/400  loss=0.7069\n",
      "Epoch 52/400  loss=0.7219\n",
      "Epoch 53/400  loss=0.8120\n",
      "Epoch 54/400  loss=0.7671\n",
      "Epoch 55/400  loss=0.7742\n",
      "Epoch 56/400  loss=0.7793\n",
      "Epoch 57/400  loss=0.6047\n",
      "Epoch 58/400  loss=0.7008\n",
      "Epoch 59/400  loss=0.6160\n",
      "Epoch 60/400  loss=0.6607\n",
      "Epoch 61/400  loss=0.7262\n",
      "Epoch 62/400  loss=0.8448\n",
      "Epoch 63/400  loss=0.7409\n",
      "Epoch 64/400  loss=0.7131\n",
      "Epoch 65/400  loss=0.8006\n",
      "Epoch 66/400  loss=0.7085\n",
      "Epoch 67/400  loss=0.6562\n",
      "Epoch 68/400  loss=0.7026\n",
      "Epoch 69/400  loss=0.8497\n",
      "Epoch 70/400  loss=0.7021\n",
      "Epoch 71/400  loss=0.7235\n",
      "Epoch 72/400  loss=0.6611\n",
      "Epoch 73/400  loss=0.5717\n",
      "Epoch 74/400  loss=0.7421\n",
      "Epoch 75/400  loss=0.6504\n",
      "Epoch 76/400  loss=0.7602\n",
      "Epoch 77/400  loss=0.8330\n",
      "Epoch 78/400  loss=0.8096\n",
      "Epoch 79/400  loss=0.5774\n",
      "Epoch 80/400  loss=0.6317\n",
      "Epoch 81/400  loss=0.7250\n",
      "Epoch 82/400  loss=0.7368\n",
      "Epoch 83/400  loss=0.6992\n",
      "Epoch 84/400  loss=0.7400\n",
      "Epoch 85/400  loss=0.6488\n",
      "Epoch 86/400  loss=0.7697\n",
      "Epoch 87/400  loss=0.6159\n",
      "Epoch 88/400  loss=0.8051\n",
      "Epoch 89/400  loss=0.7643\n",
      "Epoch 90/400  loss=0.7546\n",
      "Epoch 91/400  loss=0.7483\n",
      "Epoch 92/400  loss=0.5926\n",
      "Epoch 93/400  loss=0.8030\n",
      "Epoch 94/400  loss=0.6123\n",
      "Epoch 95/400  loss=0.8021\n",
      "Epoch 96/400  loss=0.8364\n",
      "Epoch 97/400  loss=0.6485\n",
      "Epoch 98/400  loss=0.7083\n",
      "Epoch 99/400  loss=0.5911\n",
      "Epoch 100/400  loss=0.8556\n",
      "Epoch 101/400  loss=0.6777\n",
      "Epoch 102/400  loss=0.6772\n",
      "Epoch 103/400  loss=0.6424\n",
      "Epoch 104/400  loss=0.6874\n",
      "Epoch 105/400  loss=0.8454\n",
      "Epoch 106/400  loss=0.7084\n",
      "Epoch 107/400  loss=0.6460\n",
      "Epoch 108/400  loss=0.6234\n",
      "Epoch 109/400  loss=0.7002\n",
      "Epoch 110/400  loss=0.6644\n",
      "Epoch 111/400  loss=0.6892\n",
      "Epoch 112/400  loss=0.7530\n",
      "Epoch 113/400  loss=0.6548\n",
      "Epoch 114/400  loss=0.7001\n",
      "Epoch 115/400  loss=0.5592\n",
      "Epoch 116/400  loss=0.7897\n",
      "Epoch 117/400  loss=0.7736\n",
      "Epoch 118/400  loss=0.8184\n",
      "Epoch 119/400  loss=0.7855\n",
      "Epoch 120/400  loss=0.5884\n",
      "Epoch 121/400  loss=0.7351\n",
      "Epoch 122/400  loss=0.7630\n",
      "Epoch 123/400  loss=0.7287\n",
      "Epoch 124/400  loss=0.6742\n",
      "Epoch 125/400  loss=0.7046\n",
      "Epoch 126/400  loss=0.6644\n",
      "Epoch 127/400  loss=0.6141\n",
      "Epoch 128/400  loss=0.6392\n",
      "Epoch 129/400  loss=0.7208\n",
      "Epoch 130/400  loss=0.6591\n",
      "Epoch 131/400  loss=0.6048\n",
      "Epoch 132/400  loss=0.6201\n",
      "Epoch 133/400  loss=0.7771\n",
      "Epoch 134/400  loss=0.7899\n",
      "Epoch 135/400  loss=0.6090\n",
      "Epoch 136/400  loss=0.7809\n",
      "Epoch 137/400  loss=0.7330\n",
      "Epoch 138/400  loss=0.9208\n",
      "Epoch 139/400  loss=0.7259\n",
      "Epoch 140/400  loss=0.8528\n",
      "Epoch 141/400  loss=0.6906\n",
      "Epoch 142/400  loss=0.5092\n",
      "Epoch 143/400  loss=0.7295\n",
      "Epoch 144/400  loss=0.7230\n",
      "Epoch 145/400  loss=0.6825\n",
      "Epoch 146/400  loss=0.7487\n",
      "Epoch 147/400  loss=0.8114\n",
      "Epoch 148/400  loss=0.6481\n",
      "Epoch 149/400  loss=0.6869\n",
      "Epoch 150/400  loss=0.6227\n",
      "Epoch 151/400  loss=0.8305\n",
      "Epoch 152/400  loss=0.7160\n",
      "Epoch 153/400  loss=0.7410\n",
      "Epoch 154/400  loss=0.8576\n",
      "Epoch 155/400  loss=0.7505\n",
      "Epoch 156/400  loss=0.6155\n",
      "Epoch 157/400  loss=0.5446\n",
      "Epoch 158/400  loss=0.5381\n",
      "Epoch 159/400  loss=0.7755\n",
      "Epoch 160/400  loss=0.7272\n",
      "Epoch 161/400  loss=0.6018\n",
      "Epoch 162/400  loss=0.6787\n",
      "Epoch 163/400  loss=0.7705\n",
      "Epoch 164/400  loss=0.6965\n",
      "Epoch 165/400  loss=0.5934\n",
      "Epoch 166/400  loss=0.8193\n",
      "Epoch 167/400  loss=0.7530\n",
      "Epoch 168/400  loss=0.8359\n",
      "Epoch 169/400  loss=0.6431\n",
      "Epoch 170/400  loss=0.8660\n",
      "Epoch 171/400  loss=0.7422\n",
      "Epoch 172/400  loss=0.6325\n",
      "Epoch 173/400  loss=0.7068\n",
      "Epoch 174/400  loss=0.5646\n",
      "Epoch 175/400  loss=0.7343\n",
      "Epoch 176/400  loss=0.8931\n",
      "Epoch 177/400  loss=0.8109\n",
      "Epoch 178/400  loss=0.7414\n",
      "Epoch 179/400  loss=0.7373\n",
      "Epoch 180/400  loss=0.7301\n",
      "Epoch 181/400  loss=0.7684\n",
      "Epoch 182/400  loss=0.6465\n",
      "Epoch 183/400  loss=0.8518\n",
      "Epoch 184/400  loss=0.5786\n",
      "Epoch 185/400  loss=0.8698\n",
      "Epoch 186/400  loss=0.6342\n",
      "Epoch 187/400  loss=0.4813\n",
      "Epoch 188/400  loss=0.6208\n",
      "Epoch 189/400  loss=0.5899\n",
      "Epoch 190/400  loss=0.7889\n",
      "Epoch 191/400  loss=0.6493\n",
      "Epoch 192/400  loss=0.7498\n",
      "Epoch 193/400  loss=0.6158\n",
      "Epoch 194/400  loss=0.6169\n",
      "Epoch 195/400  loss=0.7098\n",
      "Epoch 196/400  loss=0.6486\n",
      "Epoch 197/400  loss=0.6641\n",
      "Epoch 198/400  loss=0.7135\n",
      "Epoch 199/400  loss=0.6310\n",
      "Epoch 200/400  loss=0.6077\n",
      "Epoch 201/400  loss=0.6309\n",
      "Epoch 202/400  loss=0.7631\n",
      "Epoch 203/400  loss=0.6042\n",
      "Epoch 204/400  loss=0.7693\n",
      "Epoch 205/400  loss=0.6335\n",
      "Epoch 206/400  loss=0.6209\n",
      "Epoch 207/400  loss=0.8191\n",
      "Epoch 208/400  loss=0.6685\n",
      "Epoch 209/400  loss=0.6309\n",
      "Epoch 210/400  loss=0.7115\n",
      "Epoch 211/400  loss=0.5750\n",
      "Epoch 212/400  loss=0.5979\n",
      "Epoch 213/400  loss=0.5812\n",
      "Epoch 214/400  loss=0.6377\n",
      "Epoch 215/400  loss=0.8001\n",
      "Epoch 216/400  loss=0.8210\n",
      "Epoch 217/400  loss=0.6277\n",
      "Epoch 218/400  loss=0.7326\n",
      "Epoch 219/400  loss=0.7247\n",
      "Epoch 220/400  loss=0.6629\n",
      "Epoch 221/400  loss=0.6844\n",
      "Epoch 222/400  loss=0.6886\n",
      "Epoch 223/400  loss=0.6749\n",
      "Epoch 224/400  loss=0.7472\n",
      "Epoch 225/400  loss=0.7373\n",
      "Epoch 226/400  loss=0.6898\n",
      "Epoch 227/400  loss=0.7131\n",
      "Epoch 228/400  loss=0.7478\n",
      "Epoch 229/400  loss=0.6654\n",
      "Epoch 230/400  loss=0.6866\n",
      "Epoch 231/400  loss=0.7746\n",
      "Epoch 232/400  loss=0.4826\n",
      "Epoch 233/400  loss=0.7607\n",
      "Epoch 234/400  loss=0.6770\n",
      "Epoch 235/400  loss=0.8803\n",
      "Epoch 236/400  loss=0.6995\n",
      "Epoch 237/400  loss=0.7153\n",
      "Epoch 238/400  loss=0.6745\n",
      "Epoch 239/400  loss=0.7409\n",
      "Epoch 240/400  loss=0.7497\n",
      "Epoch 241/400  loss=0.7819\n",
      "Epoch 242/400  loss=0.6294\n",
      "Epoch 243/400  loss=0.5996\n",
      "Epoch 244/400  loss=0.6451\n",
      "Epoch 245/400  loss=0.6063\n",
      "Epoch 246/400  loss=0.7621\n",
      "Epoch 247/400  loss=0.8023\n",
      "Epoch 248/400  loss=0.6874\n",
      "Epoch 249/400  loss=0.6394\n",
      "Epoch 250/400  loss=0.7051\n",
      "Epoch 251/400  loss=0.7628\n",
      "Epoch 252/400  loss=0.5899\n",
      "Epoch 253/400  loss=0.7223\n",
      "Epoch 254/400  loss=0.6601\n",
      "Epoch 255/400  loss=0.7130\n",
      "Epoch 256/400  loss=0.6187\n",
      "Epoch 257/400  loss=0.7911\n",
      "Epoch 258/400  loss=0.6002\n",
      "Epoch 259/400  loss=0.6825\n",
      "Epoch 260/400  loss=0.6774\n",
      "Epoch 261/400  loss=0.7492\n",
      "Epoch 262/400  loss=0.7110\n",
      "Epoch 263/400  loss=0.5810\n",
      "Epoch 264/400  loss=0.6903\n",
      "Epoch 265/400  loss=0.7425\n",
      "Epoch 266/400  loss=0.7067\n",
      "Epoch 267/400  loss=0.5010\n",
      "Epoch 268/400  loss=0.7103\n",
      "Epoch 269/400  loss=0.7563\n",
      "Epoch 270/400  loss=0.7217\n",
      "Epoch 271/400  loss=0.7331\n",
      "Epoch 272/400  loss=0.7529\n",
      "Epoch 273/400  loss=0.6314\n",
      "Epoch 274/400  loss=0.6346\n",
      "Epoch 275/400  loss=0.7571\n",
      "Epoch 276/400  loss=0.7309\n",
      "Epoch 277/400  loss=0.6516\n",
      "Epoch 278/400  loss=0.8037\n",
      "Epoch 279/400  loss=0.6503\n",
      "Epoch 280/400  loss=0.6333\n",
      "Epoch 281/400  loss=0.7756\n",
      "Epoch 282/400  loss=0.8292\n",
      "Epoch 283/400  loss=0.8032\n",
      "Epoch 284/400  loss=0.5885\n",
      "Epoch 285/400  loss=0.7677\n",
      "Epoch 286/400  loss=0.5920\n",
      "Epoch 287/400  loss=0.7329\n",
      "Epoch 288/400  loss=0.7142\n",
      "Epoch 289/400  loss=0.7639\n",
      "Epoch 290/400  loss=0.6461\n",
      "Epoch 291/400  loss=0.7228\n",
      "Epoch 292/400  loss=0.7636\n",
      "Epoch 293/400  loss=0.7675\n",
      "Epoch 294/400  loss=0.7632\n",
      "Epoch 295/400  loss=0.7316\n",
      "Epoch 296/400  loss=0.7314\n",
      "Epoch 297/400  loss=0.7408\n",
      "Epoch 298/400  loss=0.6533\n",
      "Epoch 299/400  loss=0.7555\n",
      "Epoch 300/400  loss=0.6900\n",
      "Epoch 301/400  loss=0.7280\n",
      "Epoch 302/400  loss=0.7656\n",
      "Epoch 303/400  loss=0.6592\n",
      "Epoch 304/400  loss=0.6972\n",
      "Epoch 305/400  loss=0.6985\n",
      "Epoch 306/400  loss=0.6296\n",
      "Epoch 307/400  loss=0.6199\n",
      "Epoch 308/400  loss=0.7229\n",
      "Epoch 309/400  loss=0.7149\n",
      "Epoch 310/400  loss=0.5471\n",
      "Epoch 311/400  loss=0.7220\n",
      "Epoch 312/400  loss=0.8293\n",
      "Epoch 313/400  loss=0.6542\n",
      "Epoch 314/400  loss=0.7115\n",
      "Epoch 315/400  loss=0.7200\n",
      "Epoch 316/400  loss=0.6939\n",
      "Epoch 317/400  loss=0.7545\n",
      "Epoch 318/400  loss=0.6190\n",
      "Epoch 319/400  loss=0.6900\n",
      "Epoch 320/400  loss=0.8302\n",
      "Epoch 321/400  loss=0.6164\n",
      "Epoch 322/400  loss=0.5242\n",
      "Epoch 323/400  loss=0.7367\n",
      "Epoch 324/400  loss=0.6544\n",
      "Epoch 325/400  loss=0.6827\n",
      "Epoch 326/400  loss=0.7803\n",
      "Epoch 327/400  loss=0.6974\n",
      "Epoch 328/400  loss=0.6825\n",
      "Epoch 329/400  loss=0.6411\n",
      "Epoch 330/400  loss=0.5355\n",
      "Epoch 331/400  loss=0.7459\n",
      "Epoch 332/400  loss=0.6860\n",
      "Epoch 333/400  loss=0.7427\n",
      "Epoch 334/400  loss=0.6346\n",
      "Epoch 335/400  loss=0.7255\n",
      "Epoch 336/400  loss=0.6832\n",
      "Epoch 337/400  loss=0.7603\n",
      "Epoch 338/400  loss=0.6289\n",
      "Epoch 339/400  loss=0.6533\n",
      "Epoch 340/400  loss=0.6606\n",
      "Epoch 341/400  loss=0.7254\n",
      "Epoch 342/400  loss=0.5938\n",
      "Epoch 343/400  loss=0.8805\n",
      "Epoch 344/400  loss=0.6876\n",
      "Epoch 345/400  loss=0.6145\n",
      "Epoch 346/400  loss=0.7160\n",
      "Epoch 347/400  loss=0.5968\n",
      "Epoch 348/400  loss=0.7769\n",
      "Epoch 349/400  loss=0.7696\n",
      "Epoch 350/400  loss=0.6994\n",
      "Epoch 351/400  loss=0.7795\n",
      "Epoch 352/400  loss=0.6762\n",
      "Epoch 353/400  loss=0.8003\n",
      "Epoch 354/400  loss=0.7091\n",
      "Epoch 355/400  loss=0.7709\n",
      "Epoch 356/400  loss=0.5692\n",
      "Epoch 357/400  loss=0.7637\n",
      "Epoch 358/400  loss=0.6981\n",
      "Epoch 359/400  loss=0.7145\n",
      "Epoch 360/400  loss=0.5485\n",
      "Epoch 361/400  loss=0.6696\n",
      "Epoch 362/400  loss=0.4588\n",
      "Epoch 363/400  loss=0.6749\n",
      "Epoch 364/400  loss=0.6163\n",
      "Epoch 365/400  loss=0.6437\n",
      "Epoch 366/400  loss=0.5755\n",
      "Epoch 367/400  loss=0.6871\n",
      "Epoch 368/400  loss=0.7179\n",
      "Epoch 369/400  loss=0.7330\n",
      "Epoch 370/400  loss=0.6617\n",
      "Epoch 371/400  loss=0.6319\n",
      "Epoch 372/400  loss=0.7314\n",
      "Epoch 373/400  loss=0.7194\n",
      "Epoch 374/400  loss=0.6643\n",
      "Epoch 375/400  loss=0.6435\n",
      "Epoch 376/400  loss=0.5929\n",
      "Epoch 377/400  loss=0.7655\n",
      "Epoch 378/400  loss=0.6482\n",
      "Epoch 379/400  loss=0.6940\n",
      "Epoch 380/400  loss=0.7362\n",
      "Epoch 381/400  loss=0.7420\n",
      "Epoch 382/400  loss=0.7046\n",
      "Epoch 383/400  loss=0.6535\n",
      "Epoch 384/400  loss=0.7980\n",
      "Epoch 385/400  loss=0.9185\n",
      "Epoch 386/400  loss=0.7478\n",
      "Epoch 387/400  loss=0.7546\n",
      "Epoch 388/400  loss=0.5446\n",
      "Epoch 389/400  loss=0.5842\n",
      "Epoch 390/400  loss=0.5323\n",
      "Epoch 391/400  loss=0.7847\n",
      "Epoch 392/400  loss=0.7095\n",
      "Epoch 393/400  loss=0.6248\n",
      "Epoch 394/400  loss=0.5542\n",
      "Epoch 395/400  loss=0.6964\n",
      "Epoch 396/400  loss=0.6816\n",
      "Epoch 397/400  loss=0.7521\n",
      "Epoch 398/400  loss=0.6244\n",
      "Epoch 399/400  loss=0.6509\n",
      "Epoch 400/400  loss=0.7262\n",
      "Saved generated trajectories: (200, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Imports & Config\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import torch, math, random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "K = 20\n",
    "input_path   = f'traj_{K}.npy'\n",
    "output_path  = f'traj_{K}_generated_seq_transformer.npy'\n",
    "BATCH_SIZE   = 10\n",
    "EPOCHS       = 400\n",
    "T_STEPS      = 50          # diffusion steps\n",
    "LR           = 2e-4\n",
    "EMBED_DIM    = 64\n",
    "N_LAYERS     = 4\n",
    "N_HEADS      = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_R   = 2\n",
    "NUM_TOK = K * NUM_R        # 每个 time-step 的组合 token 种数\n",
    "# =========================================================\n",
    "# 2. Data : (N, L, 2) → (N, L) integer tokens\n",
    "# =========================================================\n",
    "raw = np.load(input_path)               # (N, L, 2)\n",
    "SEQ_LEN = raw.shape[1]\n",
    "print('Loaded', raw.shape[0], 'trajectories of length', SEQ_LEN)\n",
    "a_arr = raw[:, :, 0].astype(np.int64)\n",
    "r_arr = raw[:, :, 1].astype(np.int64)\n",
    "tok_arr = (a_arr * NUM_R + r_arr).astype(np.int64)   # (N, L)\n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, tokens):\n",
    "        self.tok = torch.from_numpy(tokens).long()\n",
    "    def __len__(self):\n",
    "        return self.tok.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tok[idx]          # (L,)\n",
    "dataloader = DataLoader(TrajDataset(tok_arr), batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=True)\n",
    "# =========================================================\n",
    "# 3. Discrete forward diffusion helper (token → token)\n",
    "# =========================================================\n",
    "def forward_diffusion(x0, betas, num_classes):\n",
    "    \"\"\"x0 : (B, L) LongTensor → list len T+1, each (B, L)\"\"\"\n",
    "    traj = [x0]\n",
    "    x_prev = x0\n",
    "    for beta in betas:\n",
    "        mask  = (torch.rand_like(x_prev.float()) < beta)\n",
    "        noise = torch.randint(0, num_classes, x_prev.shape, device=x_prev.device)\n",
    "        x_next = torch.where(mask, noise, x_prev)\n",
    "        traj.append(x_next)\n",
    "        x_prev = x_next\n",
    "    return traj\n",
    "# =========================================================\n",
    "# 4. Model : sequence‐aware discrete diffusion network\n",
    "# =========================================================\n",
    "class SeqDiscreteDiffusion(nn.Module):\n",
    "    def __init__(self, num_tok=NUM_TOK, seq_len=SEQ_LEN, embed_dim=EMBED_DIM, n_layers=N_LAYERS, n_heads=N_HEADS):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_tok, embed_dim)\n",
    "        self.pos_emb   = nn.Embedding(seq_len, embed_dim)\n",
    "        self.time_emb  = nn.Embedding(1000, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, dim_feedforward=embed_dim*4, activation='relu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(embed_dim, num_tok)\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t : (B, L)  LongTensor\n",
    "        t   : (B,)    LongTensor  (same t for the whole seq)\n",
    "        \"\"\"\n",
    "        B, L = x_t.shape\n",
    "        tok  = self.token_emb(x_t)                         # (B,L,E)\n",
    "        pos  = self.pos_emb(torch.arange(L, device=x_t.device))  # (L,E)\n",
    "        pos  = pos.unsqueeze(0).expand(B, -1, -1)          # (B,L,E)\n",
    "        time = self.time_emb(t).unsqueeze(1).expand(-1, L, -1)   # (B,L,E)\n",
    "        h = tok + pos + time                               # (B,L,E)\n",
    "        h = self.transformer(h)                            # (B,L,E)\n",
    "        logits = self.head(h)                              # (B,L,NUM_TOK)\n",
    "        return logits\n",
    "# =========================================================\n",
    "# 5. Training loop\n",
    "# =========================================================\n",
    "betas = [0.1] * T_STEPS\n",
    "model = SeqDiscreteDiffusion().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for x0 in dataloader:          # x0 : (B,L)\n",
    "        x0 = x0.to(device)\n",
    "        B, L = x0.shape\n",
    "        # forward diffusion once per batch (global t for the whole seq)\n",
    "        traj  = forward_diffusion(x0, betas, NUM_TOK)      # list len T+1\n",
    "        t_bar = torch.randint(1, T_STEPS + 1, (B,), device=device)  # (B,)\n",
    "        x_t   = torch.stack([traj[t][i]   for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        x_prev= torch.stack([traj[t-1][i] for i,t in enumerate(t_bar)])  # (B,L)\n",
    "        logits = model(x_t, t_bar)                         # (B,L,NUM_TOK)\n",
    "        loss   = ce(logits.reshape(-1, NUM_TOK), x_prev.reshape(-1))\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}  loss={loss.item():.4f}')\n",
    "# =========================================================\n",
    "# 6. Sampling (reverse diffusion)\n",
    "# =========================================================\n",
    "NUM_SAMPLES = 100   # 可自行调整\n",
    "def sample(model, n_samples=NUM_SAMPLES):\n",
    "    model.eval()\n",
    "    B = n_samples\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randint(0, NUM_TOK, (B, SEQ_LEN), device=device)\n",
    "        for t in reversed(range(1, T_STEPS + 1)):\n",
    "            t_vec = torch.full((B,), t, device=device)\n",
    "            logits = model(x_t, t_vec)                     # (B,L,NUM_TOK)\n",
    "            probs  = torch.softmax(logits, dim=-1)\n",
    "            x_t    = torch.multinomial(probs.view(-1, NUM_TOK), 1).squeeze(-1).view(B, SEQ_LEN)      # (B,L)\n",
    "        return x_t.cpu()                                   # (B,L)\n",
    "gen_tok = sample(model, NUM_SAMPLES)                       # (N,L)\n",
    "# 解码回 (a,r) 二元组\n",
    "gen_a = (gen_tok // NUM_R).numpy()\n",
    "gen_r = (gen_tok %  NUM_R).numpy()\n",
    "gen_traj = np.stack([gen_a, gen_r], axis=-1)               # (N,L,2)\n",
    "# concat the training data `raw`` and the generated `data gen_traj`\n",
    "trajs = np.concatenate([raw, gen_traj], axis=0)          # (N+200, L, 2)\n",
    "np.save(output_path, trajs)\n",
    "print('Saved generated trajectories:', trajs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
