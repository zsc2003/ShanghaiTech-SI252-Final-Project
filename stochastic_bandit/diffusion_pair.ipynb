{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37338516",
   "metadata": {},
   "source": [
    "# Discrete Diffusion on 2‑D Trajectories `(a, r)`\n",
    "\n",
    "This notebook is a **drop‑in replacement** for your original *discrete.ipynb*.\n",
    "It keeps the same cell order and variable names as much as possible, but now\n",
    "treats `a ∈ {0,…,19}` and `r ∈ {0,1}` as **two coupled chains** instead of\n",
    "packing them into one token.\n",
    "\n",
    "**Expected data** : a NumPy file `traj.npy` of shape `(N, L, 2)` where the last\n",
    "dimension holds `(a, r)` pairs.\n",
    "Run the notebook top‑to‑bottom to train and sample new trajectories; outputs are\n",
    "saved as `generated_trajs.npy` with shape `(B, L, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfaa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, math, random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---- Config ----\n",
    "K = 20\n",
    "input_path   = f'traj_{K}.npy'\n",
    "output_path  = f'traj_{K}_generated_pair.npy'\n",
    "BATCH_SIZE   = 64\n",
    "EPOCHS       = 30\n",
    "SEQ_LEN      = None         # will infer after loading data\n",
    "T_STEPS      = 12           # diffusion steps\n",
    "LR           = 1e-3\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec004b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 trajectories of length 50\n"
     ]
    }
   ],
   "source": [
    "raw = np.load(input_path)\n",
    "SEQ_LEN = raw.shape[1]\n",
    "print('Loaded', raw.shape[0], 'trajectories of length', SEQ_LEN)\n",
    "\n",
    "# Split into two integer tensors\n",
    "a_arr = raw[:, :, 0].astype(np.int64)   # (N, L)\n",
    "r_arr = raw[:, :, 1].astype(np.int64)   # (N, L)\n",
    "\n",
    "NUM_A = K   # classes for a\n",
    "NUM_R = 2    # classes for r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74689171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, a_data, r_data):\n",
    "        self.a = torch.from_numpy(a_data).long()\n",
    "        self.r = torch.from_numpy(r_data).long()\n",
    "    def __len__(self):\n",
    "        return self.a.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'a': self.a[idx],   # (L,)\n",
    "            'r': self.r[idx],   # (L,)\n",
    "        }\n",
    "\n",
    "dataset = TrajDataset(a_arr, r_arr)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297a4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x0_flat, betas, num_classes):\n",
    "    \"\"\"Return list [x_0, x_1, ..., x_T]\"\"\"\n",
    "    traj = [x0_flat]\n",
    "    x_prev = x0_flat\n",
    "    for beta in betas:\n",
    "        mask = torch.rand_like(x_prev.float()) < beta\n",
    "        noise = torch.randint(0, num_classes, x_prev.shape, device=x_prev.device)\n",
    "        x_next = torch.where(mask, noise, x_prev)\n",
    "        traj.append(x_next)\n",
    "        x_prev = x_next\n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646ca7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteDiffusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_emb = nn.Embedding(1000, time_emb_dim)\n",
    "        in_dim = NUM_A + NUM_R + time_emb_dim\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "        )\n",
    "        self.head_a = nn.Linear(hidden_dim, NUM_A)\n",
    "        self.head_r = nn.Linear(hidden_dim, NUM_R)\n",
    "\n",
    "    def forward(self, a_t, r_t, t):\n",
    "        # a_t, r_t: [B*L] LongTensor; t: [B*L]\n",
    "        a_one = torch.zeros(a_t.size(0), NUM_A, device=a_t.device)\n",
    "        a_one.scatter_(1, a_t.unsqueeze(1), 1.)\n",
    "        r_one = torch.zeros(r_t.size(0), NUM_R, device=r_t.device)\n",
    "        r_one.scatter_(1, r_t.unsqueeze(1), 1.)\n",
    "        h = torch.cat([a_one, r_one, self.time_emb(t)], dim=1)\n",
    "        h = self.backbone(h)\n",
    "        return self.head_a(h), self.head_r(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106815f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=3.6819\n",
      "Epoch 2/30  loss=3.6559\n",
      "Epoch 3/30  loss=3.6343\n",
      "Epoch 4/30  loss=3.6175\n",
      "Epoch 5/30  loss=3.6046\n",
      "Epoch 6/30  loss=3.5825\n",
      "Epoch 7/30  loss=3.5665\n",
      "Epoch 8/30  loss=3.5610\n",
      "Epoch 9/30  loss=3.5356\n",
      "Epoch 10/30  loss=3.5427\n",
      "Epoch 11/30  loss=3.5406\n",
      "Epoch 12/30  loss=3.5126\n",
      "Epoch 13/30  loss=3.4952\n",
      "Epoch 14/30  loss=3.5001\n",
      "Epoch 15/30  loss=3.4783\n",
      "Epoch 16/30  loss=3.4675\n",
      "Epoch 17/30  loss=3.4519\n",
      "Epoch 18/30  loss=3.4442\n",
      "Epoch 19/30  loss=3.4396\n",
      "Epoch 20/30  loss=3.4043\n",
      "Epoch 21/30  loss=3.4149\n",
      "Epoch 22/30  loss=3.3905\n",
      "Epoch 23/30  loss=3.3748\n",
      "Epoch 24/30  loss=3.3581\n",
      "Epoch 25/30  loss=3.3448\n",
      "Epoch 26/30  loss=3.3198\n",
      "Epoch 27/30  loss=3.2995\n",
      "Epoch 28/30  loss=3.2838\n",
      "Epoch 29/30  loss=3.2490\n",
      "Epoch 30/30  loss=3.2453\n"
     ]
    }
   ],
   "source": [
    "betas = [0.1] * T_STEPS\n",
    "model = DiscreteDiffusion().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        a0 = batch['a'].to(device)    # (B,L)\n",
    "        r0 = batch['r'].to(device)\n",
    "        B, L = a0.shape\n",
    "        a0_flat = a0.reshape(-1)\n",
    "        r0_flat = r0.reshape(-1)\n",
    "\n",
    "        traj_a = forward_diffusion(a0_flat, betas, NUM_A)\n",
    "        traj_r = forward_diffusion(r0_flat, betas, NUM_R)\n",
    "\n",
    "        t_bar = torch.randint(1, T_STEPS + 1, (B*L,), device=device)\n",
    "        a_t = torch.stack([traj_a[t][i] for i, t in enumerate(t_bar)])\n",
    "        r_t = torch.stack([traj_r[t][i] for i, t in enumerate(t_bar)])\n",
    "        a_prev = torch.stack([traj_a[t-1][i] for i, t in enumerate(t_bar)])\n",
    "        r_prev = torch.stack([traj_r[t-1][i] for i, t in enumerate(t_bar)])\n",
    "\n",
    "        logits_a, logits_r = model(a_t, r_t, t_bar)\n",
    "        loss = ce(logits_a, a_prev) + ce(logits_r, r_prev)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}  loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6908c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated_trajs torch.Size([200, 50, 2])\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 100   # <‑‑‑ set this to any N you like (e.g. 1000)\n",
    "\n",
    "def sample(model, n_samples=NUM_SAMPLES):\n",
    "    \"\"\"Return tensor (N, SEQ_LEN, 2) on CPU.\"\"\"\n",
    "    model.eval()\n",
    "    BATCH = n_samples\n",
    "    with torch.no_grad():\n",
    "        a_t = torch.randint(0, NUM_A, (BATCH, SEQ_LEN), device=device)\n",
    "        r_t = torch.randint(0, NUM_R, (BATCH, SEQ_LEN), device=device)\n",
    "        for t in reversed(range(1, T_STEPS + 1)):\n",
    "            t_vec = torch.full((BATCH * SEQ_LEN,), t, device=device)\n",
    "            logits_a, logits_r = model(a_t.reshape(-1), r_t.reshape(-1), t_vec)\n",
    "            probs_a = torch.softmax(logits_a, dim=-1)\n",
    "            probs_r = torch.softmax(logits_r, dim=-1)\n",
    "            a_t = torch.multinomial(probs_a, 1).squeeze(-1).reshape(BATCH, SEQ_LEN)\n",
    "            r_t = torch.multinomial(probs_r, 1).squeeze(-1).reshape(BATCH, SEQ_LEN)\n",
    "        trajs = torch.stack([a_t.cpu(), r_t.cpu()], dim=-1)\n",
    "        return trajs  # (N, L, 2)\n",
    "\n",
    "trajs = sample(model, NUM_SAMPLES)\n",
    "# concat the training data and the generated data\n",
    "trajs = torch.cat([torch.from_numpy(raw), trajs], dim=0)\n",
    "np.save(f'{output_path}', trajs.numpy())\n",
    "print('Saved generated_trajs', trajs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
